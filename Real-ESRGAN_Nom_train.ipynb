{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO List\n",
    "- Real-ESRGAN with new loss functions\n",
    "- Train for 2x upscale - for faster training and suitable for small dataset of Nom\n",
    "- Using Chinese text dataset: HWBD 1.1, Real-CE, ToK1871"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup, Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essential\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "# logger\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "from torch.nn import init as init\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "\n",
    "from torchvision.models import vgg as vgg\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
    "    \"\"\"Initialize network weights.\n",
    "\n",
    "    Args:\n",
    "        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
    "        scale (float): Scale initialized weights, especially for residual\n",
    "            blocks. Default: 1.\n",
    "        bias_fill (float): The value to fill bias. Default: 0\n",
    "        kwargs (dict): Other arguments for initialization function.\n",
    "    \"\"\"\n",
    "    if not isinstance(module_list, list):\n",
    "        module_list = [module_list]\n",
    "    for module in module_list:\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, **kwargs)\n",
    "                m.weight.data *= scale\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(bias_fill)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, **kwargs)\n",
    "                m.weight.data *= scale\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(bias_fill)\n",
    "            elif isinstance(m, _BatchNorm):\n",
    "                init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(bias_fill)\n",
    "                    \n",
    "def make_layer(basic_block, num_basic_block, **kwarg):\n",
    "    \"\"\"Make layers by stacking the same blocks.\n",
    "\n",
    "    Args:\n",
    "        basic_block (nn.module): nn.module class for basic block.\n",
    "        num_basic_block (int): number of blocks.\n",
    "\n",
    "    Returns:\n",
    "        nn.Sequential: Stacked blocks in nn.Sequential.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_basic_block):\n",
    "        layers.append(basic_block(**kwarg))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def pixel_unshuffle(x, scale):\n",
    "    \"\"\" Pixel unshuffle.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input feature with shape (b, c, hh, hw).\n",
    "        scale (int): Downsample ratio.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: the pixel unshuffled feature.\n",
    "    \"\"\"\n",
    "    b, c, hh, hw = x.size()\n",
    "    out_channel = c * (scale**2)\n",
    "    assert hh % scale == 0 and hw % scale == 0\n",
    "    h = hh // scale\n",
    "    w = hw // scale\n",
    "    x_view = x.view(b, c, h, scale, w, scale)\n",
    "    return x_view.permute(0, 1, 3, 5, 2, 4).reshape(b, out_channel, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Feature Extractor\n",
    "For use in Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_PRETRAIN_PATH = 'experiments/pretrained_models/vgg19-dcbb9e9d.pth'\n",
    "NAMES = {\n",
    "    'vgg11': [\n",
    "        'conv1_1', 'relu1_1', 'pool1', 'conv2_1', 'relu2_1', 'pool2', 'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2',\n",
    "        'pool3', 'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'pool4', 'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2',\n",
    "        'pool5'\n",
    "    ],\n",
    "    'vgg13': [\n",
    "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'pool3', 'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'pool4',\n",
    "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'pool5'\n",
    "    ],\n",
    "    'vgg16': [\n",
    "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'pool3', 'conv4_1', 'relu4_1', 'conv4_2',\n",
    "        'relu4_2', 'conv4_3', 'relu4_3', 'pool4', 'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3',\n",
    "        'pool5'\n",
    "    ],\n",
    "    'vgg19': [\n",
    "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3', 'conv4_1',\n",
    "        'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4', 'conv5_1', 'relu5_1',\n",
    "        'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4', 'pool5'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def insert_bn(names):\n",
    "    \"\"\"Insert bn layer after each conv.\n",
    "\n",
    "    Args:\n",
    "        names (list): The list of layer names.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of layer names with bn layers.\n",
    "    \"\"\"\n",
    "    names_bn = []\n",
    "    for name in names:\n",
    "        names_bn.append(name)\n",
    "        if 'conv' in name:\n",
    "            position = name.replace('conv', '')\n",
    "            names_bn.append('bn' + position)\n",
    "    return names_bn\n",
    "\n",
    "\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    \"\"\"VGG network for feature extraction.\n",
    "\n",
    "    In this implementation, we allow users to choose whether use normalization\n",
    "    in the input feature and the type of vgg network. Note that the pretrained\n",
    "    path must fit the vgg type.\n",
    "\n",
    "    Args:\n",
    "        layer_name_list (list[str]): Forward function returns the corresponding\n",
    "            features according to the layer_name_list.\n",
    "            Example: {'relu1_1', 'relu2_1', 'relu3_1'}.\n",
    "        vgg_type (str): Set the type of vgg network. Default: 'vgg19'.\n",
    "        use_input_norm (bool): If True, normalize the input image. Importantly,\n",
    "            the input feature must in the range [0, 1]. Default: True.\n",
    "        range_norm (bool): If True, norm images with range [-1, 1] to [0, 1].\n",
    "            Default: False.\n",
    "        requires_grad (bool): If true, the parameters of VGG network will be\n",
    "            optimized. Default: False.\n",
    "        remove_pooling (bool): If true, the max pooling operations in VGG net\n",
    "            will be removed. Default: False.\n",
    "        pooling_stride (int): The stride of max pooling operation. Default: 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 layer_name_list,\n",
    "                 vgg_type='vgg19',\n",
    "                 use_input_norm=True,\n",
    "                 range_norm=False,\n",
    "                 requires_grad=False,\n",
    "                 remove_pooling=False,\n",
    "                 pooling_stride=2):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.layer_name_list = layer_name_list\n",
    "        self.use_input_norm = use_input_norm\n",
    "        self.range_norm = range_norm\n",
    "\n",
    "        self.names = NAMES[vgg_type.replace('_bn', '')]\n",
    "        if 'bn' in vgg_type:\n",
    "            self.names = insert_bn(self.names)\n",
    "\n",
    "        # only borrow layers that will be used to avoid unused params\n",
    "        max_idx = 0\n",
    "        for v in layer_name_list:\n",
    "            idx = self.names.index(v)\n",
    "            if idx > max_idx:\n",
    "                max_idx = idx\n",
    "\n",
    "        if os.path.exists(VGG_PRETRAIN_PATH):\n",
    "            vgg_net = getattr(vgg, vgg_type)(pretrained=False)\n",
    "            state_dict = torch.load(VGG_PRETRAIN_PATH, map_location=lambda storage, loc: storage)\n",
    "            vgg_net.load_state_dict(state_dict)\n",
    "        else:\n",
    "            vgg_net = getattr(vgg, vgg_type)(pretrained=True)\n",
    "\n",
    "        features = vgg_net.features[:max_idx + 1]\n",
    "\n",
    "        modified_net = OrderedDict()\n",
    "        for k, v in zip(self.names, features):\n",
    "            if 'pool' in k:\n",
    "                # if remove_pooling is true, pooling operation will be removed\n",
    "                if remove_pooling:\n",
    "                    continue\n",
    "                else:\n",
    "                    # in some cases, we may want to change the default stride\n",
    "                    modified_net[k] = nn.MaxPool2d(kernel_size=2, stride=pooling_stride)\n",
    "            else:\n",
    "                modified_net[k] = v\n",
    "\n",
    "        self.vgg_net = nn.Sequential(modified_net)\n",
    "\n",
    "        if not requires_grad:\n",
    "            self.vgg_net.eval()\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            self.vgg_net.train()\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        if self.use_input_norm:\n",
    "            # the mean is for image with range [0, 1]\n",
    "            self.register_buffer('mean', torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "            # the std is for image with range [0, 1]\n",
    "            self.register_buffer('std', torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor with shape (n, c, h, w).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Forward results.\n",
    "        \"\"\"\n",
    "        if self.range_norm:\n",
    "            x = (x + 1) / 2\n",
    "        if self.use_input_norm:\n",
    "            x = (x - self.mean) / self.std\n",
    "\n",
    "        output = {}\n",
    "        for key, layer in self.vgg_net._modules.items():\n",
    "            x = layer(x)\n",
    "            if key in self.layer_name_list:\n",
    "                output[key] = x.clone()\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-ESRGAN Blind Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealESRGANPairedDataset(Dataset):\n",
    "    \"\"\"Dataset used for Real-ESRGAN model:\n",
    "    Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data.\n",
    "\n",
    "    It loads gt (Ground-Truth) images, and augments them.\n",
    "    It also generates blur kernels and sinc kernels for applying on low-quality images on the fly.\n",
    "    Note that the low-quality images are processed in tensors on GPUS for faster processing.\n",
    "    \"\"\"\n",
    "    # TODO: Implement later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedImageDataset(Dataset):\n",
    "    '''Paired image dataset for image restoration, super-resolution.\n",
    "    Read LQ (Low Quality, e.g. LR (Low Resolution), blurry, noisy, etc) and GT image pairs.\n",
    "\n",
    "    list of options:\n",
    "    - scale (int): scale factor for the input image\n",
    "    - dataroot_gt (str): path to the ground truth image folder\n",
    "    - dataroot_lq (str): path to the low quality image folder\n",
    "    - preprocess (transforms.Compose): preprocessing function to apply to the image\n",
    "\n",
    "    Created for training ESRGAN model.\n",
    "    '''\n",
    "    def __init__(self, opt):\n",
    "        self.scale = opt['scale']\n",
    "        self.preprocess = opt['preprocess']\n",
    "        self.gt_folder, self.lq_folder = opt['dataroot_gt'], opt['dataroot_lq']\n",
    "        self.paths = {\n",
    "            'gt_path': [],\n",
    "            'lq_path': []\n",
    "        }\n",
    "\n",
    "        def paired_paths_from_folder(folders):\n",
    "            lq_folder, gt_folder = folders\n",
    "            lq_paths = os.listdir(lq_folder)\n",
    "            gt_paths = os.listdir(gt_folder)\n",
    "            assert len(lq_paths) == len(gt_paths), (f'LQ and GT datasets have different number of images: 'f'{len(lq_paths)}, {len(gt_paths)}.')\n",
    "            return lq_paths, gt_paths\n",
    "        \n",
    "        self.paths['lq_path'], self.paths['gt_path'] = paired_paths_from_folder([self.lq_folder, self.gt_folder])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths['gt_path'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_gt = cv2.imread(os.path.join(self.gt_folder, self.paths['gt_path'][idx]))\n",
    "        img_lq = cv2.imread(os.path.join(self.lq_folder, self.paths['lq_path'][idx]))\n",
    "\n",
    "        img_gt = self.preprocess(img_gt)\n",
    "        img_lq = self.preprocess(img_lq)\n",
    "\n",
    "        return {'lq': img_lq, 'gt': img_gt}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Architecture: RRDBNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, num_feat=64, num_grow_ch=32):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_feat, num_grow_ch, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(num_feat + num_grow_ch, num_grow_ch, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(num_feat + 2 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(num_feat + 3 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(num_feat + 4 * num_grow_ch, num_feat, 3, 1, 1)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # initialization\n",
    "        default_init_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        # Empirically, we use 0.2 to scale the residual for better performance\n",
    "        return x5 * 0.2 + x\n",
    "    \n",
    "class RRDB(nn.Module):\n",
    "    \"\"\"Residual in Residual Dense Block.\n",
    "\n",
    "    Used in RRDB-Net in ESRGAN.\n",
    "\n",
    "    Args:\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "        num_grow_ch (int): Channels for each growth.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feat, num_grow_ch=32):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.rdb1 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
    "        self.rdb2 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
    "        self.rdb3 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.rdb1(x)\n",
    "        out = self.rdb2(out)\n",
    "        out = self.rdb3(out)\n",
    "        # Empirically, we use 0.2 to scale the residual for better performance\n",
    "        return out * 0.2 + x\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    \"\"\"Networks consisting of Residual in Residual Dense Block, which is used\n",
    "    in ESRGAN as generator.\n",
    "\n",
    "    ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks.\n",
    "\n",
    "    We extend ESRGAN for scale x2 and scale x1.\n",
    "    Note: This is one option for scale 1, scale 2 in RRDBNet.\n",
    "    We first employ the pixel-unshuffle (an inverse operation of pixelshuffle to reduce the spatial size\n",
    "    and enlarge the channel size before feeding inputs into the main ESRGAN architecture.\n",
    "\n",
    "    Args:\n",
    "        num_in_ch (int): Channel number of inputs.\n",
    "        num_out_ch (int): Channel number of outputs.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "            Default: 64\n",
    "        num_block (int): Block number in the trunk network. Defaults: 23\n",
    "        num_grow_ch (int): Channels for each growth. Default: 32.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_in_ch, num_out_ch, scale=4, num_feat=64, num_block=23, num_grow_ch=32):\n",
    "        super(RRDBNet, self).__init__()\n",
    "        self.scale = scale\n",
    "        if scale == 2:\n",
    "            num_in_ch = num_in_ch * 4\n",
    "        elif scale == 1:\n",
    "            num_in_ch = num_in_ch * 16\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1)\n",
    "        self.body = make_layer(RRDB, num_block, num_feat=num_feat, num_grow_ch=num_grow_ch)\n",
    "        self.conv_body = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        # upsample\n",
    "        self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.scale == 2:\n",
    "            feat = pixel_unshuffle(x, scale=2)\n",
    "        elif self.scale == 1:\n",
    "            feat = pixel_unshuffle(x, scale=4)\n",
    "        else:\n",
    "            feat = x\n",
    "        feat = self.conv_first(feat)\n",
    "        body_feat = self.conv_body(self.body(feat))\n",
    "        feat = feat + body_feat\n",
    "        # upsample\n",
    "        feat = self.lrelu(self.conv_up1(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        feat = self.lrelu(self.conv_up2(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        out = self.conv_last(self.lrelu(self.conv_hr(feat)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Architecture: VGG19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGStyleDiscriminator(nn.Module):\n",
    "    \"\"\"VGG style discriminator with input size 128 x 128 or 256 x 256.\n",
    "\n",
    "    It is used to train SRGAN, ESRGAN, and VideoGAN.\n",
    "\n",
    "    Args:\n",
    "        num_in_ch (int): Channel number of inputs. Default: 3.\n",
    "        num_feat (int): Channel number of base intermediate features.Default: 64.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_in_ch, num_feat, input_size=128):\n",
    "        super(VGGStyleDiscriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        assert self.input_size == 128 or self.input_size == 256, (\n",
    "            f'input size must be 128 or 256, but received {input_size}')\n",
    "\n",
    "        self.conv0_0 = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1, bias=True)\n",
    "        self.conv0_1 = nn.Conv2d(num_feat, num_feat, 4, 2, 1, bias=False)\n",
    "        self.bn0_1 = nn.BatchNorm2d(num_feat, affine=True)\n",
    "\n",
    "        self.conv1_0 = nn.Conv2d(num_feat, num_feat * 2, 3, 1, 1, bias=False)\n",
    "        self.bn1_0 = nn.BatchNorm2d(num_feat * 2, affine=True)\n",
    "        self.conv1_1 = nn.Conv2d(num_feat * 2, num_feat * 2, 4, 2, 1, bias=False)\n",
    "        self.bn1_1 = nn.BatchNorm2d(num_feat * 2, affine=True)\n",
    "\n",
    "        self.conv2_0 = nn.Conv2d(num_feat * 2, num_feat * 4, 3, 1, 1, bias=False)\n",
    "        self.bn2_0 = nn.BatchNorm2d(num_feat * 4, affine=True)\n",
    "        self.conv2_1 = nn.Conv2d(num_feat * 4, num_feat * 4, 4, 2, 1, bias=False)\n",
    "        self.bn2_1 = nn.BatchNorm2d(num_feat * 4, affine=True)\n",
    "\n",
    "        self.conv3_0 = nn.Conv2d(num_feat * 4, num_feat * 8, 3, 1, 1, bias=False)\n",
    "        self.bn3_0 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
    "        self.conv3_1 = nn.Conv2d(num_feat * 8, num_feat * 8, 4, 2, 1, bias=False)\n",
    "        self.bn3_1 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
    "\n",
    "        self.conv4_0 = nn.Conv2d(num_feat * 8, num_feat * 8, 3, 1, 1, bias=False)\n",
    "        self.bn4_0 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
    "        self.conv4_1 = nn.Conv2d(num_feat * 8, num_feat * 8, 4, 2, 1, bias=False)\n",
    "        self.bn4_1 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
    "\n",
    "        if self.input_size == 256:\n",
    "            self.conv5_0 = nn.Conv2d(num_feat * 8, num_feat * 8, 3, 1, 1, bias=False)\n",
    "            self.bn5_0 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
    "            self.conv5_1 = nn.Conv2d(num_feat * 8, num_feat * 8, 4, 2, 1, bias=False)\n",
    "            self.bn5_1 = nn.BatchNorm2d(num_feat * 8, affine=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(num_feat * 8 * 4 * 4, 100)\n",
    "        self.linear2 = nn.Linear(100, 1)\n",
    "\n",
    "        # activation function\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.size(2) == self.input_size, (f'Input size must be identical to input_size, but received {x.size()}.')\n",
    "\n",
    "        feat = self.lrelu(self.conv0_0(x))\n",
    "        feat = self.lrelu(self.bn0_1(self.conv0_1(feat)))  # output spatial size: /2\n",
    "\n",
    "        feat = self.lrelu(self.bn1_0(self.conv1_0(feat)))\n",
    "        feat = self.lrelu(self.bn1_1(self.conv1_1(feat)))  # output spatial size: /4\n",
    "\n",
    "        feat = self.lrelu(self.bn2_0(self.conv2_0(feat)))\n",
    "        feat = self.lrelu(self.bn2_1(self.conv2_1(feat)))  # output spatial size: /8\n",
    "\n",
    "        feat = self.lrelu(self.bn3_0(self.conv3_0(feat)))\n",
    "        feat = self.lrelu(self.bn3_1(self.conv3_1(feat)))  # output spatial size: /16\n",
    "\n",
    "        feat = self.lrelu(self.bn4_0(self.conv4_0(feat)))\n",
    "        feat = self.lrelu(self.bn4_1(self.conv4_1(feat)))  # output spatial size: /32\n",
    "\n",
    "        if self.input_size == 256:\n",
    "            feat = self.lrelu(self.bn5_0(self.conv5_0(feat)))\n",
    "            feat = self.lrelu(self.bn5_1(self.conv5_1(feat)))  # output spatial size: / 64\n",
    "\n",
    "        # spatial size: (4, 4)\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "        feat = self.lrelu(self.linear1(feat))\n",
    "        out = self.linear2(feat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Architecture: U-Net with Spectral Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDiscriminatorSN(nn.Module):\n",
    "    \"\"\"Defines a U-Net discriminator with spectral normalization (SN)\n",
    "\n",
    "    It is used in Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data.\n",
    "\n",
    "    Arg:\n",
    "        num_in_ch (int): Channel number of inputs. Default: 3.\n",
    "        num_feat (int): Channel number of base intermediate features. Default: 64.\n",
    "        skip_connection (bool): Whether to use skip connections between U-Net. Default: True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_in_ch, num_feat=64, skip_connection=True):\n",
    "        super(UNetDiscriminatorSN, self).__init__()\n",
    "        self.skip_connection = skip_connection\n",
    "        norm = spectral_norm\n",
    "        # the first convolution\n",
    "        self.conv0 = nn.Conv2d(num_in_ch, num_feat, kernel_size=3, stride=1, padding=1)\n",
    "        # downsample\n",
    "        self.conv1 = norm(nn.Conv2d(num_feat, num_feat * 2, 4, 2, 1, bias=False))\n",
    "        self.conv2 = norm(nn.Conv2d(num_feat * 2, num_feat * 4, 4, 2, 1, bias=False))\n",
    "        self.conv3 = norm(nn.Conv2d(num_feat * 4, num_feat * 8, 4, 2, 1, bias=False))\n",
    "        # upsample\n",
    "        self.conv4 = norm(nn.Conv2d(num_feat * 8, num_feat * 4, 3, 1, 1, bias=False))\n",
    "        self.conv5 = norm(nn.Conv2d(num_feat * 4, num_feat * 2, 3, 1, 1, bias=False))\n",
    "        self.conv6 = norm(nn.Conv2d(num_feat * 2, num_feat, 3, 1, 1, bias=False))\n",
    "        # extra convolutions\n",
    "        self.conv7 = norm(nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=False))\n",
    "        self.conv8 = norm(nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=False))\n",
    "        self.conv9 = nn.Conv2d(num_feat, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsample\n",
    "        x0 = F.leaky_relu(self.conv0(x), negative_slope=0.2, inplace=True)\n",
    "        x1 = F.leaky_relu(self.conv1(x0), negative_slope=0.2, inplace=True)\n",
    "        x2 = F.leaky_relu(self.conv2(x1), negative_slope=0.2, inplace=True)\n",
    "        x3 = F.leaky_relu(self.conv3(x2), negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # upsample\n",
    "        x3 = F.interpolate(x3, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x4 = F.leaky_relu(self.conv4(x3), negative_slope=0.2, inplace=True)\n",
    "\n",
    "        if self.skip_connection:\n",
    "            x4 = x4 + x2\n",
    "        x4 = F.interpolate(x4, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x5 = F.leaky_relu(self.conv5(x4), negative_slope=0.2, inplace=True)\n",
    "\n",
    "        if self.skip_connection:\n",
    "            x5 = x5 + x1\n",
    "        x5 = F.interpolate(x5, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        x6 = F.leaky_relu(self.conv6(x5), negative_slope=0.2, inplace=True)\n",
    "\n",
    "        if self.skip_connection:\n",
    "            x6 = x6 + x0\n",
    "\n",
    "        # extra convolutions\n",
    "        out = F.leaky_relu(self.conv7(x6), negative_slope=0.2, inplace=True)\n",
    "        out = F.leaky_relu(self.conv8(out), negative_slope=0.2, inplace=True)\n",
    "        out = self.conv9(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Loss(nn.Module):\n",
    "    \"\"\"L1 (mean absolute error, MAE) loss.\n",
    "\n",
    "    Args:\n",
    "        loss_weight (float): Loss weight for L1 loss. Default: 1.0.\n",
    "        reduction (str): Specifies the reduction to apply to the output.\n",
    "            Supported choices are 'none' | 'mean' | 'sum'. Default: 'mean'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_weight=1.0, reduction='mean'):\n",
    "        super(L1Loss, self).__init__()\n",
    "        if reduction not in ['none', 'mean', 'sum']:\n",
    "            raise ValueError(f'Unsupported reduction mode: {reduction}. Supported ones are: {_reduction_modes}')\n",
    "\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, pred, target, weight=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred (Tensor): of shape (N, C, H, W). Predicted tensor.\n",
    "            target (Tensor): of shape (N, C, H, W). Ground truth tensor.\n",
    "            weight (Tensor, optional): of shape (N, C, H, W). Element-wise weights. Default: None.\n",
    "        \"\"\"\n",
    "        return self.loss_weight * F.l1_loss(pred, target, weight, reduction=self.reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Loss \n",
    "Implementation of Perceptual Loss from ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks  \n",
    "Also refered to as Content Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"Perceptual loss with commonly used style loss.\n",
    "\n",
    "    Args:\n",
    "        layer_weights (dict): The weight for each layer of vgg feature.\n",
    "            Here is an example: {'conv5_4': 1.}, which means the conv5_4\n",
    "            feature layer (before relu5_4) will be extracted with weight\n",
    "            1.0 in calculating losses.\n",
    "        vgg_type (str): The type of vgg network used as feature extractor.\n",
    "            Default: 'vgg19'.\n",
    "        use_input_norm (bool):  If True, normalize the input image in vgg.\n",
    "            Default: True.\n",
    "        range_norm (bool): If True, norm images with range [-1, 1] to [0, 1].\n",
    "            Default: False.\n",
    "        perceptual_weight (float): If `perceptual_weight > 0`, the perceptual\n",
    "            loss will be calculated and the loss will multiplied by the\n",
    "            weight. Default: 1.0.\n",
    "        style_weight (float): If `style_weight > 0`, the style loss will be\n",
    "            calculated and the loss will multiplied by the weight.\n",
    "            Default: 0.\n",
    "        criterion (str): Criterion used for perceptual loss. Default: 'l1'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 layer_weights,\n",
    "                 vgg_type='vgg19',\n",
    "                 use_input_norm=True,\n",
    "                 range_norm=False,\n",
    "                 perceptual_weight=1.0,\n",
    "                 style_weight=0.,\n",
    "                 criterion='l1'):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.layer_weights = layer_weights\n",
    "        self.vgg = VGGFeatureExtractor(\n",
    "            layer_name_list=list(layer_weights.keys()),\n",
    "            vgg_type=vgg_type,\n",
    "            use_input_norm=use_input_norm,\n",
    "            range_norm=range_norm)\n",
    "\n",
    "        self.criterion_type = criterion\n",
    "        if self.criterion_type == 'l1':\n",
    "            self.criterion = torch.nn.L1Loss()\n",
    "        elif self.criterion_type == 'l2':\n",
    "            self.criterion = torch.nn.MSELoss()\n",
    "        elif self.criterion_type == 'fro':\n",
    "            self.criterion = None\n",
    "        else:\n",
    "            raise NotImplementedError(f'{criterion} criterion has not been supported.')\n",
    "\n",
    "    def forward(self, x, gt):\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor with shape (n, c, h, w).\n",
    "            gt (Tensor): Ground-truth tensor with shape (n, c, h, w).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Forward results.\n",
    "        \"\"\"\n",
    "        # extract vgg features\n",
    "        x_features = self.vgg(x)\n",
    "        gt_features = self.vgg(gt.detach())\n",
    "\n",
    "        # calculate perceptual loss\n",
    "        if self.perceptual_weight > 0:\n",
    "            percep_loss = 0\n",
    "            for k in x_features.keys():\n",
    "                if self.criterion_type == 'fro':\n",
    "                    percep_loss += torch.norm(x_features[k] - gt_features[k], p='fro') * self.layer_weights[k]\n",
    "                else:\n",
    "                    percep_loss += self.criterion(x_features[k], gt_features[k]) * self.layer_weights[k]\n",
    "            percep_loss *= self.perceptual_weight\n",
    "        else:\n",
    "            percep_loss = None\n",
    "\n",
    "        # calculate style loss\n",
    "        if self.style_weight > 0:\n",
    "            style_loss = 0\n",
    "            for k in x_features.keys():\n",
    "                if self.criterion_type == 'fro':\n",
    "                    style_loss += torch.norm(\n",
    "                        self._gram_mat(x_features[k]) - self._gram_mat(gt_features[k]), p='fro') * self.layer_weights[k]\n",
    "                else:\n",
    "                    style_loss += self.criterion(self._gram_mat(x_features[k]), self._gram_mat(\n",
    "                        gt_features[k])) * self.layer_weights[k]\n",
    "            style_loss *= self.style_weight\n",
    "        else:\n",
    "            style_loss = None\n",
    "\n",
    "        return percep_loss, style_loss\n",
    "\n",
    "    def _gram_mat(self, x):\n",
    "        \"\"\"Calculate Gram matrix.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor with shape of (n, c, h, w).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Gram matrix.\n",
    "        \"\"\"\n",
    "        n, c, h, w = x.size()\n",
    "        features = x.view(n, c, w * h)\n",
    "        features_t = features.transpose(1, 2)\n",
    "        gram = features.bmm(features_t) / (c * h * w)\n",
    "        return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Prior Loss\n",
    "Implementation of proprosed GPLoss from Scene Text Image Super-Resolution in the Wild  \n",
    "https://arxiv.org/abs/2005.03341\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientProfileLoss(nn.Module):\n",
    "    ''' Gradient Profile Loss.\n",
    "    Implementation from TextZoom repository: https://github.com/WenjiaWang0312/TextZoom/blob/master/src/loss/gradient_loss.py\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): Prediction images.\n",
    "        target (Tensor): Target images.\n",
    "     \n",
    "    '''\n",
    "    def __init__(self, loss_weight=1.0, reduction='mean'):\n",
    "        super(GradientProfileLoss, self).__init__()\n",
    "        self.func = nn.L1Loss()\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_map(self, x):\n",
    "        batch_size, channel, h_x, w_x = x.size()\n",
    "        r = F.pad(x, (0, 1, 0, 0))[:, :, :, 1:]\n",
    "        l = F.pad(x, (1, 0, 0, 0))[:, :, :, :w_x]\n",
    "        t = F.pad(x, (0, 0, 1, 0))[:, :, :h_x, :]\n",
    "        b = F.pad(x, (0, 0, 0, 1))[:, :, 1:, :]\n",
    "        xgrad = torch.pow(torch.pow((r - l) * 0.5, 2) + torch.pow((t - b) * 0.5, 2), 0.5)\n",
    "        return xgrad\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        map_pred = self.gradient_map(pred)\n",
    "        map_target = self.gradient_map(target)\n",
    "        return self.func(map_pred, map_target) * self.loss_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Aware Loss\n",
    "Implementation of EALoss proposed in A Benchmark for Chinese-English Scene Text Image Super-resolution\n",
    "https://arxiv.org/abs/2308.03262"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CannyL1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CannyL1Loss(nn.Module):\n",
    "    ''' CannyL1Loss\n",
    "    From Real-CE Pixel level L1 loss on predicted and target Canny edges.\n",
    "    This is essentially a modified L1 loss, with a mask for use with Canny edge maps.\n",
    "\n",
    "    Args:\n",
    "        pred (Tensor): Prediction images, of shape (N, C, H, W).\n",
    "        target (Tensor): Target images, of shape (N, C, H, W).\n",
    "        weight (Tensor, optional): Element-wise weights, of shape (N, C, H, W). Default: None.\n",
    "        mask (Tensor, optional): Mask for sth?\n",
    "    '''\n",
    "    def __init__(self, loss_weight=1.0, reduction='mean', **kwargs):\n",
    "        super(CannyL1Loss, self).__init__()\n",
    "        if reduction not in ['none', 'mean', 'sum']:\n",
    "            raise ValueError(f'Unsupported reduction mode: {reduction}.')\n",
    "        self.loss_weight = loss_weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def foward(self, pred, target, weight=None, mask=None, **kwargs):\n",
    "        if mask is not None:\n",
    "            # \"Foreground 0.7 and background 0.3\" is the original comment... but here they use 1.0 and 0.5 for some reason\n",
    "            a = 1.0\n",
    "            b = 0.5\n",
    "            weight_mask = weight_mask = (mask > 0).float() * a + (mask == 0).float() * b\n",
    "        else:\n",
    "            weight_mask = 1.0\n",
    "        # C = pred.shape[1]\n",
    "        return self.loss_weight * F.l1_loss(pred * weight_mask, target * weight_mask, weight, reduction=self.reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CannyPerceptualLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CannyPerceptualLoss(nn.Module):\n",
    "    ''' CannyPerceptualLoss\n",
    "    From Real-CE Perceptual loss on predicted and target Canny edges.\n",
    "    Similar with PerceptualLoss but with Canny edge map. In forward function, \n",
    "    the Canny edge map is seperated from the input image and the loss is calculated\n",
    "\n",
    "    Args:\n",
    "        layer_weights (dict): The weight for each layer of vgg feature.\n",
    "            Here is an example: {'conv5_4': 1.}, which means the conv5_4\n",
    "            feature layer (before relu5_4) will be extracted with weight\n",
    "            1.0 in calculating losses.\n",
    "        vgg_type (str): The type of vgg network used as feature extractor.\n",
    "            Default: 'vgg19'.\n",
    "        use_input_norm (bool):  If True, normalize the input image in vgg.\n",
    "            Default: True.\n",
    "        range_norm (bool): If True, norm images with range [-1, 1] to [0, 1].\n",
    "            Default: False.\n",
    "        perceptual_weight (float): If `perceptual_weight > 0`, the perceptual\n",
    "            loss will be calculated and the loss will multiplied by the\n",
    "            weight. Default: 1.0.\n",
    "        style_weight (float): If `style_weight > 0`, the style loss will be\n",
    "            calculated and the loss will multiplied by the weight.\n",
    "            Default: 0.\n",
    "        criterion (str): Criterion used for perceptual loss. Default: 'l1'.\n",
    "\n",
    "        mask (Tensor, optional): Mask for Canny edge map\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layer_weights, \n",
    "                 vgg_type='vgg19', \n",
    "                 use_input_norm=True, \n",
    "                 range_norm=False, \n",
    "                 perceptual_weight=1.0, \n",
    "                 style_weight=0., \n",
    "                 criterion='l1'):\n",
    "        super(CannyPerceptualLoss, self).__init__()\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.layer_weights = layer_weights\n",
    "        self.vgg = VGGFeatureExtractor(\n",
    "            layer_name_list=list(layer_weights.keys()),\n",
    "            vgg_type=vgg_type,\n",
    "            use_input_norm=use_input_norm,\n",
    "            range_norm=range_norm)\n",
    "\n",
    "        self.criterion_type = criterion\n",
    "        if self.criterion_type == 'l1':\n",
    "            self.criterion = torch.nn.L1Loss()\n",
    "        elif self.criterion_type == 'l2':\n",
    "            self.criterion = torch.nn.MSELoss()\n",
    "        elif self.criterion_type == 'fro':\n",
    "            self.criterion = None\n",
    "        else:\n",
    "            raise NotImplementedError(f'{criterion} criterion has not been supported.')\n",
    "\n",
    "    def forward(self, x_, gt_, mask=None):\n",
    "        # Canny weight mask\n",
    "        if mask is not None:\n",
    "            a = 1.0\n",
    "            b = 0.5\n",
    "            weight_mask = weight_mask = (mask > 0).float() * a + (mask == 0).float() * b\n",
    "        \n",
    "        \n",
    "        # the parameters have underscore at the end to avoid conflict with the original input and target\n",
    "        # seperate Canny edge map from input image, which is on 4th channel\n",
    "        x, x_canny = torch.split(x_, 3, dim=1)\n",
    "        gt, gt_canny = torch.split(gt_, 3, dim=1)\n",
    "\n",
    "        # extract vgg features\n",
    "        x_features = self.vgg(x)\n",
    "        gt_features = self.vgg(gt.detach())\n",
    "\n",
    "        # stack the Canny edge map to 3 channels, to match VGG input shape\n",
    "        x_canny = torch.stack([x_canny.squeeze(1)] * 3, dim=1)\n",
    "        gt_canny = torch.stack([gt_canny.squeeze(1)] * 3, dim=1)\n",
    "\n",
    "        x_canny_features = self.vgg(x_canny)\n",
    "        gt_canny_features = self.vgg(gt_canny.detach())\n",
    "\n",
    "        # calculate perceptual loss\n",
    "        if self.perceptual_weight > 0:\n",
    "            percep_loss = 0\n",
    "            for k in x_features.keys():\n",
    "                # rescale weight mask to match the feature map size\n",
    "                weight_mask_rescaled = F.interpolate(weight_mask, size=x_features[k].shape[2:], mode='nearest')\n",
    "\n",
    "                if self.criterion_type == 'fro':\n",
    "                    percep_loss += torch.norm(x_features[k] * x_canny_features[k] * weight_mask_rescaled \n",
    "                                              - gt_features[k] * gt_canny_features[k] * weight_mask_rescaled, p='fro') * self.layer_weights[k]\n",
    "                else:\n",
    "                    percep_loss += self.criterion(x_features[k] * x_canny_features[k] * weight_mask_rescaled, \n",
    "                                                  gt_features[k] * gt_canny_features[k] * weight_mask_rescaled) * self.layer_weights[k]\n",
    "            percep_loss *= self.perceptual_weight\n",
    "        else:\n",
    "            percep_loss = None\n",
    "\n",
    "        # calculate style loss\n",
    "        if self.style_weight > 0:\n",
    "            style_loss = 0\n",
    "            for k in x_features.keys():\n",
    "                if self.criterion_type == 'fro':\n",
    "                    style_loss += torch.norm(\n",
    "                        self._gram_mat(x_features[k]) - self._gram_mat(gt_features[k]), p='fro') * self.layer_weights[k]\n",
    "                else:\n",
    "                    style_loss += self.criterion(self._gram_mat(x_features[k]), self._gram_mat(\n",
    "                        gt_features[k])) * self.layer_weights[k]\n",
    "            style_loss *= self.style_weight\n",
    "        else:\n",
    "            style_loss = None\n",
    "\n",
    "        return percep_loss, style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Loss  \n",
    "Implement Vanilla GAN Loss (Adversarial Loss)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Define GAN loss. Also called adversarial loss.\n",
    "\n",
    "    Args:\n",
    "        gan_type (str): Support 'vanilla', 'lsgan', 'wgan', 'hinge'.\n",
    "        real_label_val (float): The value for real label. Default: 1.0.\n",
    "        fake_label_val (float): The value for fake label. Default: 0.0.\n",
    "        loss_weight (float): Loss weight. Default: 1.0.\n",
    "            Note that loss_weight is only for generators; and it is always 1.0\n",
    "            for discriminators.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0, loss_weight=1.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.gan_type = gan_type\n",
    "        self.loss_weight = loss_weight\n",
    "        self.real_label_val = real_label_val\n",
    "        self.fake_label_val = fake_label_val\n",
    "\n",
    "        # Using Vanilla, original GAN loss by Ian Goodfellow\n",
    "        if self.gan_type == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif self.gan_type == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        # elif self.gan_type == 'wgan':\n",
    "        #     self.loss = self._wgan_loss\n",
    "        # elif self.gan_type == 'wgan_softplus':\n",
    "        #     self.loss = self._wgan_softplus_loss\n",
    "        elif self.gan_type == 'hinge':\n",
    "            self.loss = nn.ReLU()\n",
    "        else:\n",
    "            raise NotImplementedError(f'GAN type {self.gan_type} is not implemented.')\n",
    "\n",
    "\n",
    "    def get_target_label(self, input, target_is_real):\n",
    "        \"\"\"Get target label.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Input tensor.\n",
    "            target_is_real (bool): Whether the target is real or fake.\n",
    "\n",
    "        Returns:\n",
    "            (bool | Tensor): Target tensor. Return bool for wgan, otherwise,\n",
    "                return Tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        target_val = (self.real_label_val if target_is_real else self.fake_label_val)\n",
    "        return input.new_ones(input.size()) * target_val\n",
    "\n",
    "    def forward(self, input, target_is_real, is_disc=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (Tensor): The input for the loss module, i.e., the network\n",
    "                prediction.\n",
    "            target_is_real (bool): Whether the targe is real or fake.\n",
    "            is_disc (bool): Whether the loss for discriminators or not.\n",
    "                Default: False.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: GAN loss value.\n",
    "        \"\"\"\n",
    "        target_label = self.get_target_label(input, target_is_real)\n",
    "        loss = self.loss(input, target_label)\n",
    "\n",
    "        # loss_weight is always 1.0 for discriminators\n",
    "        return loss if is_disc else loss * self.loss_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Val Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = dict(\n",
    "    dataset_opt=dict(\n",
    "        scale=2,\n",
    "        dataroot_gt= None,\n",
    "        dataroot_lq = None,\n",
    "        preprocess=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(90),\n",
    "            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    ),\n",
    "    model_opt = dict(\n",
    "        network_g = dict(\n",
    "            # type='RRDBNet',\n",
    "            num_in_ch=3,\n",
    "            num_out_ch=3,\n",
    "            num_feat=64,\n",
    "            num_block=23,\n",
    "            num_grow_ch=32,\n",
    "            scale=2\n",
    "        ),\n",
    "        network_d = dict(\n",
    "            # type='UNetDiscriminatorSN',\n",
    "            num_in_ch=3,\n",
    "            num_feat=64,\n",
    "            skip_connection=True\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    loss_opt = dict(\n",
    "        pixel_opt = dict(\n",
    "            # type='L1Loss',\n",
    "            loss_weight=1.0,\n",
    "            reduction='mean'\n",
    "        ),\n",
    "        perceptual_opt = dict(\n",
    "            # type='PerceptualLoss',\n",
    "            layer_weights={\n",
    "                'conv1_2': 0.1,\n",
    "                'conv2_2': 0.1,\n",
    "                'conv3_4': 1.0,\n",
    "                'conv4_4': 1.0,\n",
    "                'conv5_4': 1.0\n",
    "            },\n",
    "            vgg_type='vgg19',\n",
    "            use_input_norm=True,\n",
    "            perceptual_weight=1.0,\n",
    "            style_weight=0.0,\n",
    "            range_norm=False,\n",
    "            criterion='l1' \n",
    "        ),\n",
    "        gan_opt = dict(\n",
    "            # type='GANLoss',\n",
    "            gan_type='vanilla',\n",
    "            real_label_val=1.0,\n",
    "            fake_label_val=0.0,\n",
    "            loss_weight=0.1\n",
    "        )\n",
    "    ),\n",
    "    optimzer_opt = dict(\n",
    "        optim_g = dict(\n",
    "            # type='Adam',\n",
    "            lr=1e-4,\n",
    "            weight_decay=0.0,\n",
    "            betas=[0.9, 0.999],\n",
    "        ),\n",
    "        optim_d = dict(\n",
    "            # type='Adam',\n",
    "            lr=1e-4,\n",
    "            weight_decay=0.0,\n",
    "            betas=[0.9, 0.999],\n",
    "        )\n",
    "    ),\n",
    "    scheduler = dict(\n",
    "        # type='MultiStepLR',\n",
    "        milestones=[200, 300, 400],\n",
    "        gamma=0.5\n",
    "    ),\n",
    "    training_opt = dict(\n",
    "        total_iter=20000,\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset = PairedImageDataset(opt['dataset_opt'])\n",
    "criterion_pixel = nn.L1Loss(reduction=opt['loss_opt']['pixel_opt']['reduction'])\n",
    "criterion_content = PerceptualLoss(**opt['loss_opt']['perceptual_opt'])\n",
    "criterion_gan = GANLoss(**opt['loss_opt']['gan_opt'])\n",
    "\n",
    "model_g = RRDBNet(**opt['model_opt']['network_g'])\n",
    "model_d = UNetDiscriminatorSN(**opt['model_opt']['network_d'])\n",
    "\n",
    "optimizer_g = torch.optim.Adam(model_g.parameters(), **opt['optimzer_opt']['optim_g'])\n",
    "optimizer_d = torch.optim.Adam(model_d.parameters(), **opt['optimzer_opt']['optim_d'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
