{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5308,"status":"ok","timestamp":1719629359590,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"urZmvoSIZBs-","outputId":"d7eb25d8-f41f-4715-8b5e-c850e656b8cd"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.0 -> 24.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n","\n","[notice] A new release of pip is available: 24.0 -> 24.1\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["!pip -q install pybboxes\n","!pip -q install pytorch_lightning"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1719629881123,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"RUcKB5XsYLv7"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","import pandas as pd\n","import pickle\n","from tqdm import tqdm\n","import pybboxes as pybbx\n","from matplotlib import pyplot as plt\n","\n","\n","import torch\n","from torch.utils.data import Dataset\n","from torch import nn\n","from torch.nn import functional as F\n","from torchvision import transforms\n","\n","import pytorch_lightning as pl\n","from torchmetrics import Accuracy\n","\n","\n","from torchvision.models import resnet\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","UCODE_DICT = 'E:/Datasets/NomDataset/HWDB1.1-bitmap64-ucode-hannom-v2-tst_seen-label-set-ucode.pkl'"]},{"cell_type":"markdown","metadata":{"id":"Xxdb9dKoYLv-"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6370,"status":"ok","timestamp":1719083222454,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"6xDfMNO5YbHt","outputId":"2c5263fa-95b5-4f4c-da01-12b54da61060"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3403,"status":"ok","timestamp":1719083940241,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"hOV6S8bMYW_2","outputId":"26d35adc-d71d-476d-9a05-5c3f209d40b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","mkdir: cannot create directory ‘/root/.kaggle’: File exists\n","Dataset URL: https://www.kaggle.com/datasets/ngcthunhb/nomdataset-crops\n","License(s): unknown\n","Downloading nomdataset-crops.zip to /content\n"," 98% 178M/182M [00:01<00:00, 134MB/s]\n","100% 182M/182M [00:01<00:00, 106MB/s]\n"]}],"source":["%cd /content/\n","# !pip install kaggle\n","!mkdir -v ~/.kaggle\n","\n","!cp -f \"/content/drive/MyDrive/Thesis Resource/kaggle.json\" ~/.kaggle\n","!kaggle datasets download -d ngcthunhb/nomdataset-crops\n","!unzip -q /content/nomdataset-crops.zip -d dataset/"]},{"cell_type":"markdown","metadata":{"id":"ia7ySyDiYLwA"},"source":["## NomImageDataset - For loading raw-cropped images"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1719629329515,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"v2n3sJyGYLwB"},"outputs":[],"source":["# Dataset class for inputting YoloV5\n","class NomImageDataset(Dataset):\n","    def __init__(self, image_dir, annotation_file, unicode_dict_path, image_size=(224, 224), transform=None):\n","        self.root_dir = image_dir\n","        self.label_list = list()\n","        self.image_list = list()\n","        self.unicode_dict = dict()\n","        self.transform = transform\n","        self.image_size = image_size\n","        self.n_crop = 0\n","\n","        with open(unicode_dict_path, 'rb') as f:\n","            tmp = pickle.load(f)\n","            tmp = sorted(list(tmp.keys()))\n","        for idx, k in enumerate(tmp):\n","            self.unicode_dict[k] = idx\n","\n","        with open(annotation_file, 'r') as f:\n","            for line in tqdm(f):\n","                line = line.strip().split(',')\n","                image_name, label = line\n","                label = label.strip()\n","                image_path = os.path.join(self.root_dir, image_name)\n","\n","                self.image_list.append(image_path)\n","                try:\n","                    self.label_list.append(self.unicode_dict[label])\n","                except:\n","                    self.label_list.append(self.unicode_dict['UNK'])\n","                    # print(f'Unknown label: {label}')\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        x_image = cv2.imread(self.image_list[idx])\n","        y_label = self.label_list[idx]\n","        x_image = cv2.cvtColor(x_image, cv2.COLOR_BGR2RGB)\n","\n","        if self.transform:\n","            x_image = self.transform(x_image)\n","        else:\n","            x_image = x_image *  1.0 / 255\n","            x_image = cv2.resize(x_image, self.image_size, interpolation=cv2.INTER_LANCZOS4)\n","            # x_image = (x_image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n","            x_image = torch.from_numpy(x_image).permute(2, 0, 1).float()\n","        y_label = torch.tensor(y_label, dtype=torch.long)\n","        return x_image, y_label\n","\n","# opt = dict(\n","#     image_dir = '../NomDataset/datasets/mono-domain-datasets/tale-of-kieu/1871/1871-raw-images',\n","#     annotation_file = '../TempResources/ToK1871.txt',\n","#     unicode_dict_path = '../NomDataset/HWDB1.1-bitmap64-ucode-hannom-v2-tst-label-set-ucode.pkl',\n","#     transform = None,\n","# )\n","# dataset = NomImageDataset(**opt)\n","\n","# from matplotlib import pyplot as plt\n","# img = dataset[2][0]\n","# detBoxes = dataset[2][1]\n","\n","\n","# textLabel = []\n","# for box in detBoxes:\n","#     x_tl, y_tl, x_br, y_br, label = box\n","#     cv2.rectangle(img, (x_tl, y_tl), (x_br, y_br), (0, 255, 0), 2)\n","#     cv2.putText(img, label, (x_tl, y_tl), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","#     textLabel.append(chr(int(label, 16)))\n","# plt.imshow(img)\n","# plt.show()\n","\n","# print(textLabel)\n","\n","class ImageCropDataset(Dataset):\n","    \"\"\" Image Crop Dataset Loader, used for loading Crop images and labels of crop image\n","\n","    Args:\n","        crop_path (str): Path to the directory containing the crop images.\n","        label_path (str): Path to the file containing the labels of the crop images.\n","        input_size (tuple(int, int)): Image size to return.\n","        ucode_dict_path (str): Path to the file containing the unicode dictionary. For translate unicode to dictionary index\n","        transforms (Callable): Transforms to apply to the crop images.\n","\n","    \"\"\"\n","    def __init__(self, crop_path : str, label_path : str, input_size, ucode_dict : dict, transforms):\n","        self.crop_path = crop_path\n","        self.label_path = label_path\n","        self.ucode_dict = ucode_dict\n","        self.transforms = transforms\n","\n","        self.input_size = input_size\n","        self.num_labels = 0\n","\n","        self.crop_list = []\n","        self.labels_list = []\n","\n","        def read_crop_and_label(crop_path, label_path):\n","            with open(label_path, 'r') as f:\n","                for line in f.readlines():\n","                    line = line.split(', ')\n","                    self.labels_list.append(line[1].strip())\n","                    crop = line[0].strip()\n","                    # Check path exists\n","                    if not os.path.exists(os.path.join(crop_path, crop)):\n","                        raise FileNotFoundError(f'Crop image {os.path.join(crop_path, crop)} not found')\n","                    else:\n","                        self.crop_list.append(crop)\n","\n","        read_crop_and_label(crop_path, label_path)\n","\n","        assert self.crop_list is not None, 'No crop images found'\n","        assert self.labels_list is not None, 'No labels found'\n","        assert self.ucode_dict is not None, 'No unicode dictionary found'\n","        assert len(self.crop_list) == len(self.labels_list), 'Number of crops and labels do not match'\n","\n","        # Display statistics of dataset\n","        print(f'Number of crops: {len(self.crop_list)}')\n","        print(f'Number of labels: {self.num_labels}')\n","        print(f'Crop images shape: {self.input_size}')\n","        print(f'Number of unique labels: {len(self.ucode_dict)}')\n","\n","    def __len__(self):\n","        return len(self.crop_list)\n","\n","    def __getitem__(self, idx):\n","        assert idx < len(self), 'Index out of range'\n","        img_path = os.path.join(self.crop_path, self.crop_list[idx])\n","        x_crop_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","        h, w, _ = x_crop_img.shape\n","        if (h, w) != self.input_size:\n","            x_crop_img = cv2.resize(x_crop_img, self.input_size, cv2.INTER_LANCZOS4)\n","\n","        if transforms is not None:\n","            x_crop_img = self.transforms(x_crop_img).float()\n","        else:\n","            x_crop_img = torch.tensor(x_crop_img).float()\n","\n","        y_label = self.labels_list[idx]\n","        try:\n","            y_label = self.ucode_dict[y_label]\n","        except KeyError:\n","            # TODO: Handle unknown labels, cuz current dict does not have all Sino-Nom ucode\n","            y_label = self.ucode_dict['UNK']\n","        y_label = torch.tensor(y_label, dtype=torch.long)\n","\n","        return x_crop_img, y_label\n","\n","class ImageCropDataModule(pl.LightningDataModule):\n","    def __init__(self, data_dirs : dict, ucode_dict_path : str, input_size, batch_size : int, num_workers : int, transforms=None):\n","        super().__init__()\n","        self.data_dir = data_dirs\n","        self.ucode_dict_path = ucode_dict_path\n","        \n","        self.input_size = input_size\n","        self.transforms = transforms\n","\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        \n","        def read_ucode_dict(ucode_dict_path):\n","            with open(ucode_dict_path, 'rb') as f:\n","                ucode_dict = pickle.load(f)\n","            for i, (k, v) in enumerate(ucode_dict.items()):\n","                ucode_dict[k] = i\n","            return ucode_dict\n","        self.ucode_dict = read_ucode_dict(ucode_dict_path)\n","\n","    def setup(self, stage=None):\n","        if stage == 'fit':\n","            self.train_dataset = ImageCropDataset(self.data_dir['train'][0], self.data_dir['train'][1], self.input_size, self.ucode_dict, self.transforms)\n","            self.val_dataset = ImageCropDataset(self.data_dir['val'][0], self.data_dir['val'][1], self.input_size, self.ucode_dict, self.transforms)\n","        elif stage == 'test':\n","            self.test_dataset = ImageCropDataset(self.data_dir['test'][0], self.data_dir['test'][1], self.input_size, self.ucode_dict, self.transforms)\n","        elif stage is None:\n","            pass\n","        else:\n","            raise ValueError(f\"Stage {stage} not recognized\")\n","\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"Uckw2KiIYLwF"},"source":["# Architectures"]},{"cell_type":"markdown","metadata":{"id":"aoe10iOWYLwL"},"source":["## Recognizer : Nom_Resnet101"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1719629374773,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"_ih_ln6GYLwM"},"outputs":[],"source":["class Nom_Resnet101(nn.Module):\n","    def __init__(self, n_classes, pretrained=True):\n","        super(Nom_Resnet101, self).__init__()\n","        self.model = resnet.resnet101(weights=resnet.ResNet101_Weights.DEFAULT)\n","\n","        # Modify the last layer\n","        self.model.fc = nn.Linear(self.model.fc.in_features, n_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","class PytorchResNet101(pl.LightningModule):\n","    def __init__(self, num_labels):\n","        super(PytorchResNet101, self).__init__()\n","        self.save_hyperparameters()\n","        self.num_labels = num_labels\n","\n","        # Get ResNet architecture and remove the last FC layer\n","        backbone = resnet.resnet101(weights=resnet.ResNet101_Weights.DEFAULT)\n","        num_filters = backbone.fc.in_features\n","        layers = list(backbone.children())[:-1]\n","\n","        # Initialize layers\n","        self.feature_extractor = nn.Sequential(*layers)\n","        self.flatten = nn.Flatten()\n","        self.classifier = nn.Linear(num_filters, self.num_labels)\n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.metrics = Accuracy(task=\"multiclass\", num_classes=self.num_labels)\n","\n","        self.training_step_outputs = []\n","        self.validation_step_outputs = []\n","        self.test_step_outputs = []\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        x = self.flatten(x)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAzm4ZUVYLwQ"},"outputs":[],"source":["# Blank"]},{"cell_type":"markdown","metadata":{"id":"56Xc4yrvYLwQ"},"source":["# Testing"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1171,"status":"ok","timestamp":1719629569314,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"Nenw-1tcYLwS","outputId":"cc51ff19-4e90-4caf-ed16-a7da40157470"},"outputs":[{"data":{"text/plain":["Nom_Resnet101(\n","  (model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (6): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (7): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (8): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (9): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (10): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (11): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (12): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (13): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (14): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (15): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (16): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (17): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (18): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (19): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (20): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (21): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (22): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=2048, out_features=5568, bias=True)\n","  )\n",")"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["unicode_dict = dict()\n","with open(UCODE_DICT, 'rb') as f:\n","    temp = pickle.load(f)\n","for idx, (k, v) in enumerate(temp.items()):\n","    unicode_dict[idx] = k\n","\n","\n","# Load the recognizer model\n","recognizer_model = Nom_Resnet101(n_classes=len(unicode_dict.keys()))\n","recognizer_model.model.load_state_dict(torch.load('E:/Github/Thesis/Backup/pretrained_model/NomResnet101.pth'))\n","recognizer_model.eval()\n"]},{"cell_type":"markdown","metadata":{"id":"z_p1AQqq65ug"},"source":["## Test on Raw images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217930,"status":"ok","timestamp":1719633487976,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"v_bxwOhS65ui","outputId":"651ac198-9351-4ba5-e7d5-90d1c9f657d8"},"outputs":[],"source":["# Load raw dataset\n","DATASET_NAME = 'LVT'\n","dataset = NomImageDataset(\n","    image_dir = f'E:/Datasets/TempResources/{DATASET_NAME}/{DATASET_NAME}_crops',\n","    annotation_file = f'E:/Datasets/TempResources/{DATASET_NAME}/{DATASET_NAME}_crops.txt',\n","    unicode_dict_path = UCODE_DICT,\n","    # scale=SCALE,\n","    image_size=(32, 32),\n","    transform = None,\n",")\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n","\n","label_dict = dict()\n","with open(UCODE_DICT, 'rb') as f:\n","    tmp = pickle.load(f)\n","for idx, (k, v) in enumerate(tmp.items()):\n","    label_dict[idx] = k\n","\n","torch.cuda.empty_cache()\n","recognizer_model.to(DEVICE)\n","\n","pbar = tqdm(total=len(dataloader), desc='Testing')\n","\n","correct_pred = 0\n","incorrect_pred = []\n","for idx, (imgs, labels) in enumerate(dataloader, 1):\n","    imgs = imgs.to(DEVICE)\n","    labels = labels.to(DEVICE)\n","\n","    with torch.no_grad():\n","        # Essentially normal Resnet operation is inference on bicubic upscaled images\n","        bicubic_imgs = F.interpolate(imgs, size=(224, 224), mode='bicubic')\n","        bicubic_imgs = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(bicubic_imgs)\n","\n","\n","        preds = recognizer_model(bicubic_imgs)\n","        preds = F.softmax(preds, dim=1)\n","        preds = torch.argmax(preds, dim=1)\n","\n","        correct_pred += torch.sum(preds == labels).item()\n","        # Record failure cases\n","        for i, (pred, label) in enumerate(zip(preds, labels)):\n","            if pred != label:\n","                incorrect_pred.append((f'{idx}_{i}', pred, label))\n","        pbar.update(1)\n","\n","pbar.close()\n","print(\"\\nAccuracy:\", correct_pred / len(dataset))"]},{"cell_type":"markdown","metadata":{"id":"tx9waeAO65ui"},"source":["## Test on SR images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jupwf5Gv65uj"},"outputs":[],"source":["# Load SR dataset\n","DATASET_NAME = 'LVT'\n","dataset = NomImageDataset(\n","    image_dir = f'E:/Datasets/TempResources/{DATASET_NAME}/{DATASET_NAME}_SR_raw_crops/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x2_GAN',\n","    annotation_file = f'E:/Datasets/TempResources/{DATASET_NAME}/{DATASET_NAME}_crops.txt',\n","    unicode_dict_path = UCODE_DICT,\n","    # scale=SCALE,\n","    image_size=(32, 32),\n","    transform = None,\n",")\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n","\n","label_dict = dict()\n","with open(UCODE_DICT, 'rb') as f:\n","    tmp = pickle.load(f)\n","for idx, (k, v) in enumerate(tmp.items()):\n","    label_dict[idx] = k\n","\n","torch.cuda.empty_cache()\n","recognizer_model.to(DEVICE)\n","\n","pbar = tqdm(total=len(dataloader), desc='Testing')\n","\n","correct_pred = 0\n","incorrect_pred = []\n","for idx, (imgs, labels) in enumerate(dataloader, 1):\n","    imgs = imgs.to(DEVICE)\n","    labels = labels.to(DEVICE)\n","\n","    with torch.no_grad():\n","        # Essentially normal Resnet operation is inference on bicubic upscaled images\n","        bicubic_imgs = F.interpolate(imgs, size=(224, 224), mode='bicubic')\n","        bicubic_imgs = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(bicubic_imgs)\n","\n","\n","        preds = recognizer_model(bicubic_imgs)\n","        preds = F.softmax(preds, dim=1)\n","        preds = torch.argmax(preds, dim=1)\n","\n","        correct_pred += torch.sum(preds == labels).item()\n","        # Record failure cases\n","        for i, (pred, label) in enumerate(zip(preds, labels)):\n","            if pred != label:\n","                incorrect_pred.append((f'{idx}_{i}', pred, label))\n","        pbar.update(1)\n","\n","pbar.close()\n","print(\"/nAccuracy:\", correct_pred / len(dataset))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8eUaRP286sgN"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMDUayazjHw9cpFeCRlEpNu","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
