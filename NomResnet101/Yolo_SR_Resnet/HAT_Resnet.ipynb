{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89574,"status":"ok","timestamp":1720770628961,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"urZmvoSIZBs-","outputId":"c63ad6d8-770b-4388-bdf8-4c9bd80cbe8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install pybboxes\n","!pip -q install pytorch_lightning"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":13623,"status":"ok","timestamp":1720770642575,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"RUcKB5XsYLv7"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","import pandas as pd\n","import pickle\n","from tqdm import tqdm\n","import pybboxes as pybbx\n","from matplotlib import pyplot as plt\n","\n","\n","import torch\n","from torch.utils.data import Dataset\n","from torch import nn\n","from torch.nn import functional as F\n","from torchvision import transforms\n","\n","import pytorch_lightning as pl\n","from torchmetrics import Accuracy\n","\n","from torchvision.models import resnet\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","UCODE_DICT = 'E:/Datasets/NomDataset/HWDB1.1-bitmap64-ucode-hannom-v2-tst_seen-label-set-ucode.pkl'"]},{"cell_type":"markdown","metadata":{"id":"Xxdb9dKoYLv-"},"source":["# Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29904,"status":"ok","timestamp":1720770672469,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"6xDfMNO5YbHt","outputId":"152e8323-bba3-4ff6-f08a-354aba730d0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15996,"status":"ok","timestamp":1720770688448,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"hOV6S8bMYW_2","outputId":"caa125cb-5ed2-4b2a-f887-9b445301c440"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","mkdir: created directory '/root/.kaggle'\n","Dataset URL: https://www.kaggle.com/datasets/ngcthunhb/nomdataset-crops\n","License(s): unknown\n","Downloading nomdataset-crops.zip to /content\n"," 99% 180M/182M [00:12<00:00, 19.4MB/s]\n","100% 182M/182M [00:12<00:00, 15.4MB/s]\n"]}],"source":["%cd /content/\n","# !pip install kaggle\n","!mkdir -v ~/.kaggle\n","\n","!cp -f \"/content/drive/MyDrive/Thesis Resource/kaggle.json\" ~/.kaggle\n","!kaggle datasets download -d ngcthunhb/nomdataset-crops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gW1_O5KgZiVK"},"outputs":[],"source":["!unzip -q /content/nomdataset-crops.zip -d dataset/"]},{"cell_type":"markdown","metadata":{"id":"ia7ySyDiYLwA"},"source":["## NomImageDataset - For loading raw-cropped images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2n3sJyGYLwB"},"outputs":[],"source":["# Dataset class for inputting YoloV5\n","class NomImageDataset(Dataset):\n","    def __init__(self, image_dir, annotation_file, unicode_dict_path, image_size=(224, 224), transform=None):\n","        self.root_dir = image_dir\n","        self.label_list = list()\n","        self.image_list = list()\n","        self.unicode_dict = dict()\n","        self.transform = transform\n","        self.image_size = image_size\n","        self.n_crop = 0\n","\n","        with open(unicode_dict_path, 'rb') as f:\n","            tmp = pickle.load(f)\n","            tmp = sorted(list(tmp.keys()))\n","        for idx, k in enumerate(tmp):\n","            self.unicode_dict[k] = idx\n","\n","        with open(annotation_file, 'r') as f:\n","            for line in tqdm(f):\n","                line = line.strip().split(',')\n","                image_name, label = line\n","                label = label.strip()\n","                image_path = os.path.join(self.root_dir, image_name)\n","\n","                self.image_list.append(image_path)\n","                try:\n","                    self.label_list.append(self.unicode_dict[label])\n","                except:\n","                    self.label_list.append(self.unicode_dict['UNK'])\n","                    # print(f'Unknown label: {label}')\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        x_image = cv2.imread(self.image_list[idx])\n","        y_label = self.label_list[idx]\n","        x_image = cv2.cvtColor(x_image, cv2.COLOR_BGR2RGB)\n","\n","        if self.transform:\n","            x_image = self.transform(x_image)\n","        else:\n","            x_image = x_image *  1.0 / 255\n","            x_image = cv2.resize(x_image, self.image_size, interpolation=cv2.INTER_LANCZOS4)\n","            # x_image = (x_image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n","            x_image = torch.from_numpy(x_image).permute(2, 0, 1).float()\n","        y_label = torch.tensor(y_label, dtype=torch.long)\n","        return x_image, y_label\n","\n","# opt = dict(\n","#     image_dir = '../NomDataset/datasets/mono-domain-datasets/tale-of-kieu/1871/1871-raw-images',\n","#     annotation_file = '../TempResources/ToK1871.txt',\n","#     unicode_dict_path = '../NomDataset/HWDB1.1-bitmap64-ucode-hannom-v2-tst-label-set-ucode.pkl',\n","#     transform = None,\n","# )\n","# dataset = NomImageDataset(**opt)\n","\n","# from matplotlib import pyplot as plt\n","# img = dataset[2][0]\n","# detBoxes = dataset[2][1]\n","\n","\n","# textLabel = []\n","# for box in detBoxes:\n","#     x_tl, y_tl, x_br, y_br, label = box\n","#     cv2.rectangle(img, (x_tl, y_tl), (x_br, y_br), (0, 255, 0), 2)\n","#     cv2.putText(img, label, (x_tl, y_tl), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","#     textLabel.append(chr(int(label, 16)))\n","# plt.imshow(img)\n","# plt.show()\n","\n","# print(textLabel)\n","\n","class ImageCropDataset(Dataset):\n","    \"\"\" Image Crop Dataset Loader, used for loading Crop images and labels of crop image\n","\n","    Args:\n","        crop_path (str): Path to the directory containing the crop images.\n","        label_path (str): Path to the file containing the labels of the crop images.\n","        input_size (tuple(int, int)): Image size to return.\n","        ucode_dict_path (str): Path to the file containing the unicode dictionary. For translate unicode to dictionary index\n","        transforms (Callable): Transforms to apply to the crop images.\n","\n","    \"\"\"\n","    def __init__(self, crop_path : str, label_path : str, input_size, ucode_dict : dict, transforms):\n","        self.crop_path = crop_path\n","        self.label_path = label_path\n","        self.ucode_dict = ucode_dict\n","        self.transforms = transforms\n","\n","        self.input_size = input_size\n","        self.num_labels = 0\n","\n","        self.crop_list = []\n","        self.labels_list = []\n","\n","        def read_crop_and_label(crop_path, label_path):\n","            with open(label_path, 'r') as f:\n","                for line in f.readlines():\n","                    line = line.split(', ')\n","                    self.labels_list.append(line[1].strip())\n","                    crop = line[0].strip()\n","                    # Check path exists\n","                    if not os.path.exists(os.path.join(crop_path, crop)):\n","                        raise FileNotFoundError(f'Crop image {os.path.join(crop_path, crop)} not found')\n","                    else:\n","                        self.crop_list.append(crop)\n","\n","        read_crop_and_label(crop_path, label_path)\n","\n","        assert self.crop_list is not None, 'No crop images found'\n","        assert self.labels_list is not None, 'No labels found'\n","        assert self.ucode_dict is not None, 'No unicode dictionary found'\n","        assert len(self.crop_list) == len(self.labels_list), 'Number of crops and labels do not match'\n","\n","        # Display statistics of dataset\n","        print(f'Number of crops: {len(self.crop_list)}')\n","        print(f'Number of labels: {self.num_labels}')\n","        print(f'Crop images shape: {self.input_size}')\n","        print(f'Number of unique labels: {len(self.ucode_dict)}')\n","\n","    def __len__(self):\n","        return len(self.crop_list)\n","\n","    def __getitem__(self, idx):\n","        assert idx < len(self), 'Index out of range'\n","        img_path = os.path.join(self.crop_path, self.crop_list[idx])\n","        x_crop_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","        h, w, _ = x_crop_img.shape\n","        if (h, w) != self.input_size:\n","            x_crop_img = cv2.resize(x_crop_img, self.input_size, cv2.INTER_LANCZOS4)\n","\n","        if transforms is not None:\n","            x_crop_img = self.transforms(x_crop_img).float()\n","        else:\n","            x_crop_img = torch.tensor(x_crop_img).float()\n","\n","        y_label = self.labels_list[idx]\n","        try:\n","            y_label = self.ucode_dict[y_label]\n","        except KeyError:\n","            # TODO: Handle unknown labels, cuz current dict does not have all Sino-Nom ucode\n","            y_label = self.ucode_dict['UNK']\n","        y_label = torch.tensor(y_label, dtype=torch.long)\n","\n","        return x_crop_img, y_label\n","\n","class ImageCropDataModule(pl.LightningDataModule):\n","    def __init__(self, data_dirs : dict, ucode_dict_path : str, input_size, batch_size : int, num_workers : int, transforms=None):\n","        super().__init__()\n","        self.data_dir = data_dirs\n","        self.ucode_dict_path = ucode_dict_path\n","        \n","        self.input_size = input_size\n","        self.transforms = transforms\n","\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        \n","        def read_ucode_dict(ucode_dict_path):\n","            with open(ucode_dict_path, 'rb') as f:\n","                ucode_dict = pickle.load(f)\n","            for i, (k, v) in enumerate(ucode_dict.items()):\n","                ucode_dict[k] = i\n","            return ucode_dict\n","        self.ucode_dict = read_ucode_dict(ucode_dict_path)\n","\n","    def setup(self, stage=None):\n","        if stage == 'fit':\n","            self.train_dataset = ImageCropDataset(self.data_dir['train'][0], self.data_dir['train'][1], self.input_size, self.ucode_dict, self.transforms)\n","            self.val_dataset = ImageCropDataset(self.data_dir['val'][0], self.data_dir['val'][1], self.input_size, self.ucode_dict, self.transforms)\n","        elif stage == 'test':\n","            self.test_dataset = ImageCropDataset(self.data_dir['test'][0], self.data_dir['test'][1], self.input_size, self.ucode_dict, self.transforms)\n","        elif stage is None:\n","            pass\n","        else:\n","            raise ValueError(f\"Stage {stage} not recognized\")\n","\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"Uckw2KiIYLwF"},"source":["# Architectures"]},{"cell_type":"markdown","metadata":{"id":"aoe10iOWYLwL"},"source":["## Recognizer : Nom_Resnet101"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ih_ln6GYLwM"},"outputs":[],"source":["class Nom_Resnet101(nn.Module):\n","    def __init__(self, n_classes, pretrained=True):\n","        super(Nom_Resnet101, self).__init__()\n","        self.model = resnet.resnet101(weights=resnet.ResNet101_Weights.DEFAULT)\n","\n","        # Modify the last layer\n","        self.model.fc = nn.Linear(self.model.fc.in_features, n_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","class PytorchResNet101(pl.LightningModule):\n","    def __init__(self, num_labels):\n","        super(PytorchResNet101, self).__init__()\n","        self.save_hyperparameters()\n","        self.num_labels = num_labels\n","\n","        # Get ResNet architecture and remove the last FC layer\n","        backbone = resnet.resnet101(weights=resnet.ResNet101_Weights.DEFAULT)\n","        num_filters = backbone.fc.in_features\n","        layers = list(backbone.children())[:-1]\n","\n","        # Initialize layers\n","        self.feature_extractor = nn.Sequential(*layers)\n","        self.flatten = nn.Flatten()\n","        self.classifier = nn.Linear(num_filters, self.num_labels)\n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.metrics = Accuracy(task=\"multiclass\", num_classes=self.num_labels)\n","\n","        self.training_step_outputs = []\n","        self.validation_step_outputs = []\n","        self.test_step_outputs = []\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        x = self.flatten(x)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# Training Resnet with SR images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training Resnet with SR images\n","from data.NomDataset import ImageCropDataModule\n","\n","opt = dict(\n","    name='ToK1871_SR-RealESRGANx4_RealCE',\n","    max_epochs=100,\n","    model_mode='train',\n","    data_dirs=dict(\n","        train=[\n","            'ToK1871/ToK1871_mixedSR_crops', \n","            'ToK1871/ToK1871_mixedSR_crops.txt'\n","        ],\n","        val=[\n","            'ToK1902/ToK1902_SR_crops/Real-ESRGAN/RealESRGAN_x4plus',\n","            'ToK1902/ToK1902_crops.txt'\n","        ],\n","        test=[\n","            'LVT/LVT_SR_crops/Real-ESRGAN/RealESRGAN_x4plus',\n","            'LVT/LVT_crops.txt'\n","        ]\n","    ),\n","    ucode_dict_path='E:/Datasets/NomDataset/HWDB1.1-bitmap64-ucode-hannom-v2-tst_seen-label-set-ucode.pkl',\n","    input_size=(224, 224),\n","    transforms=transforms.Compose([\n","        transforms.ToTensor(),\n","        # transforms.RandomHorizontalFlip(),\n","        # transforms.RandomVerticalFlip(),\n","        # transforms.RandomRotation(45),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n",")\n","\n","# Create Datamodule\n","datamodule = ImageCropDataModule(data_dirs=opt['data_dirs'], ucode_dict_path=opt['ucode_dict_path'], batch_size=32, input_size=opt['input_size'], num_workers=0, transforms=opt['transforms'])\n","n_labels = len(datamodule.ucode_dict.keys())\n","print(f\"Number of labels: {n_labels}\")\n","\n","p = dict(\n","    max_epochs=50,\n","    batch_size=32,\n","    gpus=1,\n","    num_labels=n_labels,\n","    model_pretrained_path='pretrained_model/NomResnet101.pth',\n",")\n","\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath='Checkpoints/Resnet_Ckpt/',\n","    filename='Resnet_Tok1871_SR-RealESRGANx2_RealCE-{epoch:02d}-{train_acc_epoch:.4f}-{val_acc_epoch:.4f}-{train_loss_epoch:.4f}-{val_loss_epoch:.4f}',\n","    save_top_k=1,\n","    verbose=True,\n","    monitor='val_acc_epoch',\n","    mode='max',\n","    save_last=True\n",")\n","\n","early_stopping_callback = EarlyStopping(\n","    monitor='val_acc_epoch',\n","    patience=5,\n","    mode='max'\n",")\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = PytorchResNet101(num_labels=p['num_labels'])\n","model = PytorchResNet101.load_from_checkpoint(p['model_pretrained_path'], num_labels=p['num_labels'])\n","model.to(DEVICE)\n","\n","#%%\n","wandb_logger = WandbLogger(project='SR_Resnet', name=\"Resnet_SR_ToK1871-RealESRGANx2_mixedSR_wMixedBackground\")\n","tb_logger = TensorBoardLogger('tensorboard_logs', name='Resnet_SR_ToK1871-RealESRGANx2_RealCE_wMixedBackground')\n","\n","#%%\n","trainer = Trainer(\n","        accelerator='gpu',\n","        max_epochs=p['max_epochs'],\n","        callbacks=[checkpoint_callback, early_stopping_callback],\n","        logger=[wandb_logger, tb_logger],\n","    )\n","trainer.fit(model, datamodule=datamodule)\n","trainer.test(model, datamodule=datamodule)"]},{"cell_type":"markdown","metadata":{"id":"56Xc4yrvYLwQ"},"source":["# Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1291,"status":"ok","timestamp":1719629580143,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"Nenw-1tcYLwS","outputId":"230bd0cb-5939-46f8-b345-cb3f1a2e961b"},"outputs":[{"data":{"text/plain":["Nom_Resnet101(\n","  (model): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (6): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (7): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (8): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (9): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (10): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (11): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (12): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (13): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (14): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (15): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (16): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (17): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (18): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (19): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (20): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (21): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (22): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=2048, out_features=5568, bias=True)\n","  )\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["unicode_dict = dict()\n","with open(UCODE_DICT, 'rb') as f:\n","    temp = pickle.load(f)\n","for idx, (k, v) in enumerate(temp.items()):\n","    unicode_dict[idx] = k\n","\n","# # Load the SR model\n","# sr_model = RRDBNet(num_in_ch=3, num_out_ch=3, scale=4, num_feat=64, num_block=23, num_grow_ch=32)\n","# sr_model.load_state_dict(torch.load('/content/drive/MyDrive/Resource/BasicSR/PretrainedModels/Real-ESRGAN/RealESRGAN_x4plus.pth')['params_ema'])\n","# sr_model.eval()\n","\n","# Load the recognizer model\n","recognizer_model = Nom_Resnet101(n_classes=len(unicode_dict.keys()))\n","recognizer_model.model.load_state_dict(torch.load('E:/Github/Thesis/Backup/pretrained_model/NomResnet101.pth'))\n","recognizer_model.eval()\n","\n","# from torchsummary import summary\n","# summary(sr_model, (3, 56, 56), device='cpu')\n","# summary(recognizer_model, (3, 224, 224), device='cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDmOEMseg2wE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1719633287900,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"8o9Gf4uS7kGn","outputId":"6ac41066-5fd4-4318-995d-ce8256d52a90"},"outputs":[{"name":"stderr","output_type":"stream","text":["14450it [00:00, 203228.76it/s]\n"]}],"source":["# sr_model.to('cpu')\n","recognizer_model.to('cpu')\n","\n","DATASET_NAME = 'LVT'\n","dataset = NomImageDataset(\n","    image_dir = f'E:/Datasets/TempResources/LVT/LVT_SR_raw_crops/HAT_SRx4',\n","    annotation_file = f'E:/Datasets/TempResources/LVT/LVT_crops.txt',\n","    unicode_dict_path = UCODE_DICT,\n","    # scale=SCALE,\n","    image_size=(32, 32),\n","    transform = None,\n",")\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n","\n","label_dict = dict()\n","with open(UCODE_DICT, 'rb') as f:\n","    tmp = pickle.load(f)\n","for idx, (k, v) in enumerate(tmp.items()):\n","    label_dict[idx] = k\n","\n","# sample = next(iter(dataloader))\n","# plt.figure(figsize=(16, 16))\n","# for i in range(4):\n","#     plt.subplot(4, 4, i+1)\n","#     img = sample[0][i].permute(1, 2, 0).numpy() * 255\n","#     plt.imshow(sample[0][i].permute(1, 2, 0).numpy())\n","\n","#     grad_mask = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","#     thres = np.array(grad_mask).mean()\n","#     grad_mask = cv2.threshold(grad_mask, thres, 255, cv2.THRESH_BINARY)[1] / 255.0\n","#     grad_mask = torch.tensor(grad_mask).unsqueeze(0)\n","#     img_tensor = torch.cat((sample[0][i], grad_mask), dim=0).unsqueeze(0)\n","\n","#     sr_img = sr_model(img_tensor).squeeze().detach().cpu().permute(1, 2, 0).numpy()[:, :, :3]\n","#     print(sr_img.shape)\n","#     # Take first 3 channel\n","#     plt.subplot(4, 4, i+5)\n","#     plt.imshow(sr_img)\n","\n","#     # plt.title(sample[1][i].item())\n","#     try:\n","#         print(chr(int(label_dict[sample[1][i].item()], 16)), end=' ')\n","#     except:\n","#         print('UNK', end=' ')\n","# plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"z_p1AQqq65ug"},"source":["## Test on Raw images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218886,"status":"ok","timestamp":1719633508076,"user":{"displayName":"Ponpon","userId":"05605705937723541810"},"user_tz":-420},"id":"v_bxwOhS65ui","outputId":"af5f975c-41f3-4d8f-bb3e-662f83b11607"},"outputs":[{"name":"stderr","output_type":"stream","text":["Testing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 452/452 [03:38<00:00,  2.07it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy: 0.7229757785467128\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["torch.cuda.empty_cache()\n","# sr_model.to(DEVICE)\n","recognizer_model.to(DEVICE)\n","\n","pbar = tqdm(total=len(dataloader), desc='Testing')\n","\n","correct_pred = 0\n","incorrect_pred = []\n","for idx, (imgs, labels) in enumerate(dataloader, 1):\n","    imgs = imgs.to(DEVICE)\n","    labels = labels.to(DEVICE)\n","\n","    with torch.no_grad():\n","        # Essentially normal Resnet operation is inference on bicubic upscaled images\n","        bicubic_imgs = F.interpolate(imgs, size=(224, 224), mode='bicubic')\n","        bicubic_imgs = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(bicubic_imgs)\n","\n","\n","        preds = recognizer_model(bicubic_imgs)\n","        preds = F.softmax(preds, dim=1)\n","        preds = torch.argmax(preds, dim=1)\n","\n","        correct_pred += torch.sum(preds == labels).item()\n","        # Record failure cases\n","        for i, (pred, label) in enumerate(zip(preds, labels)):\n","            if pred != label:\n","                incorrect_pred.append((f'{idx}_{i}', pred, label))\n","        pbar.update(1)\n","\n","pbar.close()\n","print(\"\\nAccuracy:\", correct_pred / len(dataset))"]},{"cell_type":"markdown","metadata":{"id":"tx9waeAO65ui"},"source":["## Test on SR images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jupwf5Gv65uj"},"outputs":[],"source":["torch.cuda.empty_cache()\n","sr_model.to(DEVICE)\n","recognizer_model.to(DEVICE)\n","\n","pbar = tqdm(total=len(dataloader), desc='Testing')\n","\n","correct_pred = 0\n","incorrect_pred = []\n","for idx, (imgs, labels) in enumerate(dataloader, 1):\n","    imgs = imgs.to(DEVICE)\n","    labels = labels.to(DEVICE)\n","\n","    with torch.no_grad():\n","\n","        grayscale = imgs.mean(dim=1, keepdim=True)\n","        threshold_value = grayscale.mean()\n","        threshold_tensor = (grayscale > threshold_value).float()\n","        imgs = torch.cat([imgs, threshold_tensor], dim=1)\n","        # print(imgs.shape)\n","\n","        sr_imgs = sr_model(imgs)\n","\n","        # Remove mask channel\n","        sr_imgs = sr_imgs[:, :3, :, :]\n","        # print(sr_imgs.shape)\n","\n","\n","\n","        # Interpolate the image to 224x224\n","        sr_imgs = F.interpolate(sr_imgs, size=(224, 224), mode='bicubic')\n","        # print(sr_imgs.shape)\n","        sr_imgs = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(sr_imgs)\n","\n","\n","        preds = recognizer_model(sr_imgs)\n","        preds = F.softmax(preds, dim=1)\n","        preds = torch.argmax(preds, dim=1)\n","\n","        correct_pred += torch.sum(preds == labels).item()\n","        # Record failure cases\n","        for i, (pred, label) in enumerate(zip(preds, labels)):\n","            if pred != label:\n","                incorrect_pred.append((f'{idx}_{i}', pred, label))\n","        pbar.update(1)\n","\n","pbar.close()\n","print(\"\\nAccuracy:\", correct_pred / len(dataset))\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOQapOwnSl/J/gxiCAqXCC7","gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
