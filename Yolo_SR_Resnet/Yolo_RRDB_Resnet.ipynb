{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision.models import resnet\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "UCODE_DICT = '../NomDataset/HWDB1.1-bitmap64-ucode-hannom-v2-tst_seen-label-set-ucode.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NomImageDataset - For loading raw-cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for inputting YoloV5\n",
    "class NomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, unicode_dict_path, transform=None):\n",
    "        self.root_dir = image_dir\n",
    "        self.label_list = list()\n",
    "        self.image_list = list()\n",
    "        self.unicode_dict = dict()\n",
    "        self.transform = transform\n",
    "        self.n_crop = 0\n",
    "        \n",
    "        with open(unicode_dict_path, 'rb') as f:\n",
    "            tmp = pickle.load(f)\n",
    "            tmp = sorted(list(tmp.keys()))\n",
    "        for idx, k in enumerate(tmp):\n",
    "            self.unicode_dict[k] = idx\n",
    "        print(self.unicode_dict)\n",
    "\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split(',')\n",
    "                image_name, label = line\n",
    "                label = label.strip()\n",
    "                image_path = os.path.join(self.root_dir, image_name)\n",
    "                \n",
    "                self.image_list.append(image_path)\n",
    "                try:\n",
    "                    self.label_list.append(self.unicode_dict[label])\n",
    "                except:\n",
    "                    self.label_list.append(self.unicode_dict['UNK'])\n",
    "                    # print(f'Unknown label: {label}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_image = cv2.imread(self.image_list[idx])\n",
    "        y_label = self.label_list[idx]\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            x_image = self.transform(x_image)\n",
    "        else:\n",
    "            x_image = x_image *  1.0 / 255\n",
    "            x_image = torch.from_numpy(x_image).permute(2, 0, 1).float()\n",
    "        y_label = torch.tensor(y_label, dtype=torch.long)\n",
    "        return x_image, y_label\n",
    "\n",
    "\n",
    "# opt = dict(\n",
    "#     image_dir = '../TempResources/ToK1871/Tok1871_raw_crops',\n",
    "#     annotation_file = '../TempResources/ToK1871/ToK1871_crops.txt',\n",
    "#     unicode_dict_path = '../NomDataset/HWDB1.1-bitmap64-ucode-hannom-v2-tst_seen-label-set-ucode.pkl',\n",
    "#     transform = None,\n",
    "# )\n",
    "# dataset = NomImageDataset(**opt)\n",
    "\n",
    "# label_dict = dict()\n",
    "# with open(opt['unicode_dict_path'], 'rb') as f:\n",
    "#     tmp = pickle.load(f)\n",
    "# for idx, (k, v) in enumerate(tmp.items()):\n",
    "#     label_dict[idx] = k\n",
    "    \n",
    "\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# img = dataset[2][0].permute(1, 2, 0).numpy()\n",
    "# label = dataset[2][1]\n",
    "\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "# print(label.item())\n",
    "# print(label_dict[label.item()])\n",
    "# print(chr(int(label_dict[label.item()], 16)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NomYoloImageDataset\n",
    "Yolo inference creates new crops that doesn't have labels. This class is exclusively for finding labels of such crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybboxes as pybbx\n",
    "\n",
    "class YoloCropDataset(Dataset):\n",
    "    def __init__(self, image_file_path : str, annotation_file_path : str, label_file_path : str, unicode_dict_path : str, image_size : int | int, transform = None, scale = 1.0):\n",
    "        self.image_file_path = image_file_path\n",
    "        self.annotation_file_path = annotation_file_path\n",
    "        self.label_file_path = label_file_path\n",
    "        self.unicode_dict_path = unicode_dict_path\n",
    "        self.image_size = image_size    # Target crop image size\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.image_files = []\n",
    "        self.annotation_files = []\n",
    "        self.label_files = []\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.load_files_list()\n",
    "        \n",
    "        self.crop_dict = {'crops': [], 'original_images_name': [], 'labels': [], 'unicode_labels': []}\n",
    "        self.load_crops()        \n",
    "        \n",
    "    def load_files_list(self) -> None:\n",
    "        for file in os.listdir(self.image_file_path):\n",
    "            if file.endswith('.jpg'):\n",
    "                self.image_files.append(file)\n",
    "        for file in os.listdir(self.annotation_file_path):\n",
    "            if file.endswith('.txt'):\n",
    "                self.annotation_files.append(file)\n",
    "        assert len(self.image_files) == len(self.annotation_files), \"Number of image files and annotation files do not match\"\n",
    "        \n",
    "        for file in os.listdir(self.label_file_path):\n",
    "            if file.endswith('.xlsx'):\n",
    "                self.label_files.append(file)\n",
    "        assert len(self.image_files) == len(self.label_files), f\"Number of image files and label files do not match. {len(self.image_files)} != {len(self.label_files)}\"\n",
    "\n",
    "\n",
    "    def load_crops(self) -> None:\n",
    "        def find_best_IOU(ref_box, boxes) -> float | tuple | int:\n",
    "            def calculate_IOU(box1, box2):\n",
    "                x1, y1, x2, y2 = box1\n",
    "                x3, y3, x4, y4 = box2\n",
    "                x5, y5 = max(x1, x3), max(y1, y3)\n",
    "                x6, y6 = min(x2, x4), min(y2, y4)\n",
    "                intersection = max(0, x6 - x5) * max(0, y6 - y5)\n",
    "                area1 = (x2 - x1) * (y2 - y1)\n",
    "                area2 = (x4 - x3) * (y4 - y3)\n",
    "                union = area1 + area2 - intersection\n",
    "                return intersection / union\n",
    "            \n",
    "            best_iou = 0\n",
    "            best_box = None\n",
    "            best_index = -1\n",
    "            for index, box in enumerate(boxes, 0):\n",
    "                iou = calculate_IOU(ref_box, box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_box = box\n",
    "                    best_index = index\n",
    "            return best_iou, best_box, best_index\n",
    "        \n",
    "        \n",
    "        # Label dictionary\n",
    "        with open(self.unicode_dict_path, 'rb') as f:\n",
    "            unicode_labels = pickle.load(f)\n",
    "        for i, (k, v) in enumerate(unicode_labels.items()):\n",
    "            unicode_labels[k] = i\n",
    "        \n",
    "        # For reading yolo txt files\n",
    "        total_n = len(self.image_files)\n",
    "        for image_file, txt_file, excel_file in tqdm(zip(self.image_files, self.annotation_files, self.label_files)):\n",
    "            image = cv2.cvtColor(cv2.imread(os.path.join(self.image_file_path, image_file)), cv2.COLOR_BGR2RGB)    # Grayscale, so I can stack 3 channels later\n",
    "            h, w, _ = image.shape\n",
    "            df = pd.read_excel(os.path.join(self.label_file_path, excel_file))\n",
    "\n",
    "            label_dict = {'boxes': [], 'labels': []}\n",
    "            for _, row in df.iterrows():\n",
    "                x1, y1, x2, y2 = row['LEFT'], row['TOP'], row['RIGHT'], row['BOTTOM']\n",
    "                label = row['UNICODE']\n",
    "                \n",
    "                x1, y1, x2, y2 = x1 // self.scale, y1 // self.scale, x2 // self.scale, y2 // self.scale\n",
    "                \n",
    "                label_dict['boxes'].append((x1, y1, x2, y2))\n",
    "                label_dict['labels'].append(label)\n",
    "\n",
    "            with open(os.path.join(self.annotation_file_path, txt_file), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    _, x, y, b_w, b_h = map(float, line.split(' '))\n",
    "                    bbox = pybbx.YoloBoundingBox(x, y, b_w, b_h, image_size=(w, h)).to_voc(return_values=True)\n",
    "                    x1, y1, x2, y2 = bbox\n",
    "                    \n",
    "                    # Find the best IOU to label the cropped image\n",
    "                    iou, box, idx = find_best_IOU(bbox, label_dict['boxes'])\n",
    "                    \n",
    "                    crop_img = image[int(y1):int(y2), int(x1):int(x2)]\n",
    "                    \n",
    "                    self.crop_dict['crops'].append(crop_img)\n",
    "                    \n",
    "                    try:\n",
    "                        label = unicode_labels[label_dict['labels'][idx]]\n",
    "                    except:\n",
    "                        label = unicode_labels['UNK'] \n",
    "                    self.crop_dict['labels'].append(label)\n",
    "\n",
    "        assert len(self.crop_dict['crops']) == len(self.crop_dict['labels']), \"Number of crops and labels do not match\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.crop_dict['crops'])\n",
    "        \n",
    "    def __getitem__(self, index: int) -> torch.Tensor | torch.Tensor:\n",
    "        assert index <= len(self), \"Index out of range\"\n",
    "                \n",
    "        image = self.crop_dict['crops'][index]\n",
    "        label = self.crop_dict['labels'][index]\n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Resize the image to 224x224\n",
    "            image = cv2.resize(image, self.image_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "            image = image *  1.0 / 255\n",
    "            \n",
    "            # TODO: This is the mean and std of ImageNet dataset, need to change to the mean and std of the dataset\n",
    "            mean = [0.485, 0.456, 0.406]\n",
    "            std = [0.229, 0.224, 0.225]\n",
    "\n",
    "            # mean = [0.799, 0.818, 0.829]\n",
    "            # std = [0.183, 0.179, 0.179]\n",
    "\n",
    "            image = (image - mean) / std\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "# opt = dict(\n",
    "#     image_file_path = '../NomDataset/datasets/mono-domain-datasets/tale-of-kieu/1871/1871-raw-images',\n",
    "#     annotation_file_path = YOLO_ANNOTATION,\n",
    "#     label_file_path = '../NomDataset/datasets/mono-domain-datasets/tale-of-kieu/1871/1871-annotation/annotation-mynom',\n",
    "#     unicode_dict_path = '../NomDataset/HWDB1.1-bitmap64-ucode-hannom-v2-tst-label-set-ucode.pkl',\n",
    "#     image_size = (224, 224),\n",
    "#     transform = None,\n",
    "# )\n",
    "\n",
    "# dataset = YoloCropDataset(**opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = dataset[3][0].permute(1, 2, 0).numpy()\n",
    "# label = dataset[3][1].item()\n",
    "# from matplotlib import pyplot as plt\n",
    "# cv2.imwrite('test.jpg', img * 255)\n",
    "\n",
    "# new_unicode_dict = dict()\n",
    "# with open('../NomDataset/HWDB1.1-bitmap64-ucode-hannom-v2-tst-label-set-ucode.pkl', 'rb') as f:\n",
    "#     unicode_dict = pickle.load(f)\n",
    "# for idx, (k, v) in enumerate(unicode_dict.items()):\n",
    "#     new_unicode_dict[idx] = k\n",
    "# print(new_unicode_dict[label])\n",
    "# print(chr(int(new_unicode_dict[label], 16)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector : YoloV5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-326-gec331cbd Python-3.11.5 torch-2.2.1 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 224 layers, 7053910 parameters, 0 gradients\n",
      "WARNING  NMS time limit 0.550s exceeded\n",
      "image 1/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\1.JPG: 224x640 106 nom_chars, 97.9ms\n",
      "image 2/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\10.JPG: 256x640 39 nom_chars, 58.7ms\n",
      "image 3/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\2.JPG: 224x640 156 nom_chars, 14.6ms\n",
      "image 4/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\3.JPG: 256x640 193 nom_chars, 12.4ms\n",
      "image 5/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\4.JPG: 224x640 170 nom_chars, 20.9ms\n",
      "image 6/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\5.JPG: 224x640 168 nom_chars, 14.1ms\n",
      "image 7/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\6.JPG: 256x640 161 nom_chars, 28.0ms\n",
      "image 8/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\7.JPG: 256x640 177 nom_chars, 12.3ms\n",
      "image 9/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\8.JPG: 224x640 154 nom_chars, 20.9ms\n",
      "image 10/10 C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\TempResources\\SR_Nom_Text\\hd\\che (copy)\\9.JPG: 288x640 15 nom_chars, 88.9ms\n",
      "Speed: 1.0ms pre-process, 36.9ms inference, 77.2ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdetect\\yolo_HD_Nom\u001b[0m\n",
      "10 labels saved to detect\\yolo_HD_Nom\\labels\n"
     ]
    }
   ],
   "source": [
    "# from yolov5.models.common import DetectMultiBackend\n",
    "# from yolov5.utils.general import non_max_suppression, scale_coords, check_img_size, Profile, increment_path\n",
    "# from yolov5.utils.dataloaders import LoadImages\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# args = {\n",
    "#     'weights': '../Backup/pretrained_model/yolov5_Nom.pt',\n",
    "#     'source': '../NomDataset/datasets/mono-domain-datasets/tale-of-kieu/1871/1871-raw-images',\n",
    "#     'project': 'runs/detect',\n",
    "#     'name': 'exp',\n",
    "#     'imgsz': (640, 640),\n",
    "#     'conf_thres': 0.5,\n",
    "#     'iou_thres': 0.5,   \n",
    "#     'device': '',       # Let YOLO decide\n",
    "#     'save_txt': True,\n",
    "#     'save_crop': True,  # Save cropped prediction boxes, for debugging\n",
    "#     'exist_ok': True,\n",
    "#     'hide_labels': True,    # Hide labels from output images\n",
    "#     'hide_conf': True,      # Hide confidence, these two ommited for better visualization\n",
    "# }\n",
    "\n",
    "# # Directories\n",
    "# save_dir = increment_path(Path(args['project']) / args['name'], exist_ok=args['exist_ok'])  # increment run\n",
    "# (save_dir / 'labels' if args['save_txt'] else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "# # Load Model\n",
    "# model = DetectMultiBackend(weights=args['weights'], device=DEVICE, dnn=False, data=None, fp16=False)\n",
    "# strides, names, pt = model.strides, model.names, model.pt\n",
    "# imgsz = check_img_size((640, 640), s=strides)  # check img_size\n",
    "\n",
    "# # Dataloader\n",
    "# bs = 1\n",
    "# dataset = LoadImages(\n",
    "#     source = '../NomDataset/datasets/mono-domain-datasets/tale-of-kieu/1871/1871-raw-images',\n",
    "#     img_size = imgsz,\n",
    "#     stride = strides,\n",
    "#     auto = pt,\n",
    "#     vid_stride=1,\n",
    "# )\n",
    "\n",
    "# # Run inference\n",
    "# model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n",
    "# seen, windows, dt = 0, [], (Profile(device=DEVICE), Profile(device=DEVICE), Profile(device=DEVICE))\n",
    "\n",
    "# for path, im, im0s, vid_cap, s in dataset:\n",
    "#     with dt[0]:\n",
    "#         img = torch.from_numpy(im).to(DEVICE)\n",
    "#         im = im.half() if model.fp16 else im.float()\n",
    "#         im /= 255.0\n",
    "#         if len(im.shape) == 3:\n",
    "#             im = im[None]\n",
    "#         if model.xml and im.shape[0] > 1:\n",
    "#                 ims = torch.chunk(im, im.shape[0], 0)\n",
    "\n",
    "#         # Inference\n",
    "#         with dt[1]:\n",
    "#             visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "#             if model.xml and im.shape[0] > 1:\n",
    "#                 pred = None\n",
    "#                 for image in ims:\n",
    "#                     if pred is None:\n",
    "#                         pred = model(image, augment=False, visualize=visualize).unsqueeze(0)\n",
    "#                     else:\n",
    "#                         pred = torch.cat((pred, model(image, augment=False, visualize=visualize).unsqueeze(0)), dim=0)\n",
    "#                 pred = [pred, None]\n",
    "#             else:\n",
    "#                 pred = model(im, augment=False, visualize=visualize)\n",
    "#         # NMS\n",
    "#         with dt[2]:\n",
    "#             pred = non_max_suppression(pred, args['conf_thres'], args['iou_thres'], None, False, max_det=1000)\n",
    "\n",
    "\n",
    "\n",
    "PROJECT = './detect'\n",
    "EXP_NAME = 'yolo_tok1871'\n",
    "YOLO_RESULTS = os.path.join(PROJECT, EXP_NAME)\n",
    "YOLO_WEIGHTS = '../Backup/pretrained_model/yolov5_Nom.pt'\n",
    "SOURCE_DIR = '../TempResources/ToK1871/ToK1871_LRcubicx4'\n",
    "YOLO_CROPS = YOLO_RESULTS + '/crops/nom_char' # Yolo will save the cropped images here\n",
    "YOLO_ANNOTATION = YOLO_RESULTS + '/labels' # Yolo will save the labels here\n",
    "\n",
    "# #%%\n",
    "from yolov5.detect import run as YoloInference\n",
    "args = {\n",
    "    'weights': YOLO_WEIGHTS,\n",
    "    'source': SOURCE_DIR,\n",
    "    'imgsz': (640, 640),\n",
    "    'conf_thres': 0.5,\n",
    "    'iou_thres': 0.5,   \n",
    "    'device': '',       # Let YOLO decide\n",
    "    'save_txt': True,\n",
    "    'save_crop': True,  # Save cropped prediction boxes, for debugging\n",
    "    'project': PROJECT,\n",
    "    'name': EXP_NAME,\n",
    "    'exist_ok': False,\n",
    "    'hide_labels': True,    # Hide labels from output images\n",
    "    'hide_conf': True,      # Hide confidence, these two ommited for better visualization\n",
    "}\n",
    "YoloInference(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-326-gec331cbd Python-3.11.5 torch-2.2.1 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 224 layers, 7053910 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\Yolo_SR_Resnet\\data\\LRcubicx4\\test\\labels... 6 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6/6 [00:10<00:00,  1.79s/it]\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: C:\\Users\\Soppo\\Documents\\GitHub\\Thesis\\Yolo_SR_Resnet\\data\\LRcubicx4\\test\\labels.cache\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [00:10<00:00,  5.03s/it]\n",
      "                   all          6        618      0.995      0.694      0.779      0.505\n",
      "Speed: 0.2ms pre-process, 159.7ms inference, 4.6ms NMS per image at shape (4, 3, 640, 640)\n",
      "Results saved to \u001b[1mtest\\yolo_LRcubicx4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.9953615700307287,\n",
       "  0.6944671902600705,\n",
       "  0.7792180158933966,\n",
       "  0.5046449782428649,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " array([    0.50464]),\n",
       " (0.17305215199788412, 159.7349246342977, 4.605650901794434))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YoloV5 test and metrics\n",
    "from yolov5.val import run as YoloVal\n",
    "\n",
    "YAML_DATA = 'config.yaml'\n",
    "PROJECT = './test'\n",
    "EXP_NAME = 'yolo_LRcubicx4'\n",
    "\n",
    "args = {\n",
    "    'weights': YOLO_WEIGHTS,\n",
    "    'data': YAML_DATA,\n",
    "    'imgsz': 640,\n",
    "    'task': 'test',\n",
    "    'batch_size': 4,\n",
    "    'device': '',\n",
    "    'project': PROJECT,\n",
    "    'name': EXP_NAME,\n",
    "    'exist_ok': True,\n",
    "}\n",
    "YoloVal(**args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizer : AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizer : Nom_Resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nom_Resnet101(nn.Module):\n",
    "    def __init__(self, n_classes, pretrained=True):\n",
    "        super(Nom_Resnet101, self).__init__()\n",
    "        self.model = resnet.resnet101(weights=resnet.ResNet101_Weights.DEFAULT)\n",
    "        \n",
    "        # Modify the last layer\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-Resolution Generator: RRDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init as init\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "\n",
    "@torch.no_grad()\n",
    "def default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
    "    \"\"\"Initialize network weights.\n",
    "\n",
    "    Args:\n",
    "        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
    "        scale (float): Scale initialized weights, especially for residual\n",
    "            blocks. Default: 1.\n",
    "        bias_fill (float): The value to fill bias. Default: 0\n",
    "        kwargs (dict): Other arguments for initialization function.\n",
    "    \"\"\"\n",
    "    if not isinstance(module_list, list):\n",
    "        module_list = [module_list]\n",
    "    for module in module_list:\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, **kwargs)\n",
    "                m.weight.data *= scale\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(bias_fill)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, **kwargs)\n",
    "                m.weight.data *= scale\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(bias_fill)\n",
    "            elif isinstance(m, _BatchNorm):\n",
    "                init.constant_(m.weight, 1)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(bias_fill)\n",
    "                \n",
    "def make_layer(basic_block, num_basic_block, **kwarg):\n",
    "    \"\"\"Make layers by stacking the same blocks.\n",
    "\n",
    "    Args:\n",
    "        basic_block (nn.module): nn.module class for basic block.\n",
    "        num_basic_block (int): number of blocks.\n",
    "\n",
    "    Returns:\n",
    "        nn.Sequential: Stacked blocks in nn.Sequential.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_basic_block):\n",
    "        layers.append(basic_block(**kwarg))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def pixel_unshuffle(x, scale):\n",
    "    \"\"\" Pixel unshuffle.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input feature with shape (b, c, hh, hw).\n",
    "        scale (int): Downsample ratio.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: the pixel unshuffled feature.\n",
    "    \"\"\"\n",
    "    b, c, hh, hw = x.size()\n",
    "    out_channel = c * (scale**2)\n",
    "    assert hh % scale == 0 and hw % scale == 0\n",
    "    h = hh // scale\n",
    "    w = hw // scale\n",
    "    x_view = x.view(b, c, h, scale, w, scale)\n",
    "    return x_view.permute(0, 1, 3, 5, 2, 4).reshape(b, out_channel, h, w)\n",
    "\n",
    "\n",
    "\n",
    "class ResidualDenseBlock(nn.Module):\n",
    "    \"\"\"Residual Dense Block.\n",
    "\n",
    "    Used in RRDB block in ESRGAN.\n",
    "\n",
    "    Args:\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "        num_grow_ch (int): Channels for each growth.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feat=64, num_grow_ch=32):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_feat, num_grow_ch, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(num_feat + num_grow_ch, num_grow_ch, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(num_feat + 2 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(num_feat + 3 * num_grow_ch, num_grow_ch, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(num_feat + 4 * num_grow_ch, num_feat, 3, 1, 1)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # initialization\n",
    "        default_init_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        # Empirically, we use 0.2 to scale the residual for better performance\n",
    "        return x5 * 0.2 + x\n",
    "\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    \"\"\"Residual in Residual Dense Block.\n",
    "\n",
    "    Used in RRDB-Net in ESRGAN.\n",
    "\n",
    "    Args:\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "        num_grow_ch (int): Channels for each growth.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feat, num_grow_ch=32):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.rdb1 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
    "        self.rdb2 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
    "        self.rdb3 = ResidualDenseBlock(num_feat, num_grow_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.rdb1(x)\n",
    "        out = self.rdb2(out)\n",
    "        out = self.rdb3(out)\n",
    "        # Empirically, we use 0.2 to scale the residual for better performance\n",
    "        return out * 0.2 + x\n",
    "\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    \"\"\"Networks consisting of Residual in Residual Dense Block, which is used\n",
    "    in ESRGAN.\n",
    "\n",
    "    ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks.\n",
    "\n",
    "    We extend ESRGAN for scale x2 and scale x1.\n",
    "    Note: This is one option for scale 1, scale 2 in RRDBNet.\n",
    "    We first employ the pixel-unshuffle (an inverse operation of pixelshuffle to reduce the spatial size\n",
    "    and enlarge the channel size before feeding inputs into the main ESRGAN architecture.\n",
    "\n",
    "    Args:\n",
    "        num_in_ch (int): Channel number of inputs.\n",
    "        num_out_ch (int): Channel number of outputs.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "            Default: 64\n",
    "        num_block (int): Block number in the trunk network. Defaults: 23\n",
    "        num_grow_ch (int): Channels for each growth. Default: 32.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_in_ch, num_out_ch, scale=4, num_feat=64, num_block=23, num_grow_ch=32):\n",
    "        super(RRDBNet, self).__init__()\n",
    "        self.scale = scale\n",
    "        if scale == 2:\n",
    "            num_in_ch = num_in_ch * 4\n",
    "        elif scale == 1:\n",
    "            num_in_ch = num_in_ch * 16\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1)\n",
    "        self.body = make_layer(RRDB, num_block, num_feat=num_feat, num_grow_ch=num_grow_ch)\n",
    "        self.conv_body = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        # upsample\n",
    "        self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n",
    "        self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.scale == 2:\n",
    "            feat = pixel_unshuffle(x, scale=2)\n",
    "        elif self.scale == 1:\n",
    "            feat = pixel_unshuffle(x, scale=4)\n",
    "        else:\n",
    "            feat = x\n",
    "        feat = self.conv_first(feat)\n",
    "        body_feat = self.conv_body(self.body(feat))\n",
    "        feat = feat + body_feat\n",
    "        # upsample\n",
    "        feat = self.lrelu(self.conv_up1(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        feat = self.lrelu(self.conv_up2(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        out = self.conv_last(self.lrelu(self.conv_hr(feat)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate cell because Dataset loading is slow\n",
    "dataset = YoloCropDataset(\n",
    "    image_file_path = '../NomDataset/datasets/mono-domain-datasets/tale-of-kieu/1871/1871-raw-images',\n",
    "    annotation_file_path = YOLO_ANNOTATION,\n",
    "    label_file_path = '../NomDataset/datasets/mono-domain-datasets/tale-of-kieu/1871/1871-annotation/annotation-mynom',\n",
    "    unicode_dict_path = UCODE_DICT,\n",
    "    image_size = (56, 56),\n",
    "    transform = None,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_dict = dict()\n",
    "with open(UCODE_DICT, 'rb') as f:\n",
    "    temp = pickle.load(f)\n",
    "for idx, (k, v) in enumerate(temp.items()):\n",
    "    unicode_dict[idx] = k\n",
    "\n",
    "# Load the SR model\n",
    "sr_model = RRDBNet(num_in_ch=3, num_out_ch=3, scale=4, num_feat=64, num_block=23, num_grow_ch=32)\n",
    "sr_model.load_state_dict(torch.load('../Backup/pretrained_model/RealESRGAN_x4plus.pth')['params_ema'])\n",
    "sr_model.eval()\n",
    "\n",
    "# Load the recognizer model\n",
    "recognizer_model = Nom_Resnet101(n_classes=len(unicode_dict.keys()))\n",
    "recognizer_model.model.load_state_dict(torch.load('../Backup/pretrained_model/NomResnet101.pth'))\n",
    "recognizer_model.eval()\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(sr_model, (3, 56, 56))\n",
    "summary(recognizer_model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "batch = next(iter(dataloader))\n",
    "imgs, labels = batch\n",
    "plt.figure()\n",
    "for idx, i in enumerate(imgs, 1):\n",
    "    if idx == 17:\n",
    "        break\n",
    "    img = i.permute(1, 2, 0).numpy()\n",
    "    img = img * 255\n",
    "    img = img.clip(0, 255).astype('uint8')\n",
    "    plt.subplot(4, 4, idx)\n",
    "    plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "labels = labels.tolist()\n",
    "print(\"Labels:\", [unicode_dict[i] for i in labels][:16])\n",
    "print(\"Labels:\", end=' ')\n",
    "for idx, i in enumerate(labels[:16]):\n",
    "    if unicode_dict[i] == 'UNK':\n",
    "        print(\"UNK\", end=', ')\n",
    "    else:\n",
    "        print(chr(int(unicode_dict[i], 16)), end=', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_model.to(DEVICE)\n",
    "recognizer_model.to(DEVICE)\n",
    "\n",
    "pbar = tqdm(total=len(dataloader), desc='Testing')\n",
    "\n",
    "correct_pred = 0\n",
    "incorrect_pred = []\n",
    "for idx, (imgs, labels) in enumerate(dataloader, 1):\n",
    "    imgs = imgs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sr_imgs = sr_model(imgs)\n",
    "        preds = recognizer_model(sr_imgs)\n",
    "        preds = F.softmax(preds, dim=1)\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        \n",
    "        correct_pred += torch.sum(preds == labels).item()\n",
    "        # Record failure cases\n",
    "        for i, (pred, label) in enumerate(zip(preds, labels)):\n",
    "            if pred != label:\n",
    "                incorrect_pred.append((f'{idx}_{i}', pred, label))\n",
    "        pbar.update(1)\n",
    "        \n",
    "pbar.close()\n",
    "print(\"Accuracy:\", correct_pred / len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
