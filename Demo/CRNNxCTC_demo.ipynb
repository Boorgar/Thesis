{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://theailearner.com/2019/05/29/creating-a-crnn-model-to-recognize-text-in-an-image-part-2\n",
    "# https://keras.io/examples/vision/handwriting_recognition\n",
    "# https://github.com/pbcquoc/crnn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.append('..')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "APPROACH_NAME = 'CRNNxCTC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0': raise SystemError('GPU device not found')\n",
    "print('Found GPU at:', device_name)\n",
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp \"/content/drive/MyDrive/Thesis Resource/kaggle.json\" ~/.kaggle/\n",
    "\n",
    "!kaggle datasets download -d ngcthunhb/nomna-sr-patches\n",
    "# !kaggle datasets download -d quandang/nomnaocr\n",
    "!unzip -q /content/nomna-sr-patches.zip -d /content/NomNaDatasets\n",
    "# !unzip -q /content/nomnaocr.zip -d /content/Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /content/NomNaDatasets/NomNaOCR/SR_Patches_Real_ESRGANx2/*.txt /content/NomNaDatasets/NomNaOCR/SR_Patches_SwinIRx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = r'/content/NomNaDatasets/Patches'\n",
    "ALL_TRANSCRIPTS_PATH = f'{DATASET_DIR}/All.txt'\n",
    "VALID_TRANSCRIPTS_PATH = f'{DATASET_DIR}/Validate.txt'\n",
    "FONT_PATH = r'/content/NomNaDatasets/NomNaTong-Regular.ttf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and remove records with rare characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def ctc_decode(predictions, max_length):\n",
    "    input_length = tf.ones(len(predictions)) * predictions.shape[1]\n",
    "    preds_decoded = tf.keras.backend.ctc_decode(\n",
    "        predictions,\n",
    "        input_length = input_length,\n",
    "        greedy = True,\n",
    "    )[0][0][:, :max_length]\n",
    "\n",
    "    return tf.where(\n",
    "        preds_decoded == tf.cast(1, tf.int64),\n",
    "        tf.cast(-1, tf.int64), # Treat [UNK] token same as blank label\n",
    "        preds_decoded\n",
    "    )\n",
    "\n",
    "\n",
    "def update_tensor_column(tensor, values, col_idx):\n",
    "    if col_idx < 0: raise ValueError(\"col_idx must be >= 0\")\n",
    "    rows = tf.range(tf.shape(tensor)[0])\n",
    "    column = tf.zeros_like(rows) + col_idx\n",
    "    idxs = tf.stack([rows, column], axis=1)\n",
    "    return tf.tensor_scatter_nd_update(tensor, idxs, tf.squeeze(values, axis=-1))\n",
    "\n",
    "\n",
    "def tokens2sparse(batch_tokens):\n",
    "    idxs = tf.where(tf.logical_and(\n",
    "        batch_tokens != 0, # For [PAD] token\n",
    "        batch_tokens != -1 # For blank label if use_ctc_decode\n",
    "    ))\n",
    "    return tf.SparseTensor(\n",
    "        tf.cast(idxs, tf.int64),\n",
    "        tf.gather_nd(batch_tokens, idxs),\n",
    "        tf.cast(tf.shape(batch_tokens), tf.int64)\n",
    "    )\n",
    "\n",
    "\n",
    "def sparse2dense(tensor, shape):\n",
    "    tensor = tf.sparse.reset_shape(tensor, shape)\n",
    "    tensor = tf.sparse.to_dense(tensor, default_value=-1)\n",
    "    tensor = tf.cast(tensor, tf.float32)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def rec2csv(file_name, patch_list, data_handler, model, use_ctc_decode=False):\n",
    "    with open(file_name, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for img_path in tqdm(patch_list):\n",
    "            image = data_handler.process_image(img_path)\n",
    "            pred_tokens = model.predict(tf.expand_dims(image, axis=0))\n",
    "            pred_labels = data_handler.tokens2texts(pred_tokens, use_ctc_decode)\n",
    "            writer.writerow([img_path, pred_labels[0]])\n",
    "from string import printable\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "class DataImporter:\n",
    "    def __init__(self, dataset_dir, labels_path, min_length=4):\n",
    "        self.img_paths, self.labels = [], []\n",
    "        with open(labels_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                img_name, text = line.rstrip('\\n').split('\\t')\n",
    "                img_path = os.path.join(dataset_dir, img_name)\n",
    "                text = text.strip().lower()\n",
    "\n",
    "                if os.path.getsize(img_path) and len(text) >= min_length and self.is_clean_text(text):\n",
    "                    self.img_paths.append(img_path)\n",
    "                    self.labels.append(text)\n",
    "\n",
    "        assert len(self.img_paths) == len(self.labels), 'img_paths and labels must have same size'\n",
    "        self.img_paths = np.array(self.img_paths)\n",
    "        self.labels = np.array(self.labels)\n",
    "        self.vocabs = dict(Counter(''.join(self.labels)).most_common())\n",
    "        self.size = len(self.labels)\n",
    "\n",
    "\n",
    "    def remove_rare_chars(self, threshold=1):\n",
    "        if threshold < 2: return self\n",
    "        rare_chars, idxs_to_remove = [], []\n",
    "        is_satisfy_threshold = True\n",
    "\n",
    "        # Vocabs need to be sorted (for faster checking)\n",
    "        for char, freq in reversed(self.vocabs.items()):\n",
    "            if freq < threshold: rare_chars.append(char)\n",
    "            else: break\n",
    "\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            if any((char in label) for char in rare_chars):\n",
    "                idxs_to_remove.append(idx)\n",
    "\n",
    "        # Remove sentences containing rare characters and recalculate the vocab frequencies\n",
    "        idxs_to_remove = np.array(idxs_to_remove)\n",
    "        self.img_paths = np.delete(self.img_paths, idxs_to_remove)\n",
    "        self.labels = np.delete(self.labels, idxs_to_remove)\n",
    "\n",
    "        assert len(self.img_paths) == len(self.labels), 'img_paths and labels must have same size'\n",
    "        self.vocabs = dict(Counter(''.join(self.labels)).most_common())\n",
    "        self.size = len(self.labels)\n",
    "\n",
    "        # Check if there are still rare characters after removing sentences\n",
    "        smallest_freq = threshold + 1 # If vocabs is empty, the smallest frequency always > threshold\n",
    "        if len(self.vocabs) >= 1: smallest_freq = list(self.vocabs.values())[-1]\n",
    "        return self.remove_rare_chars(threshold) if smallest_freq < threshold else self\n",
    "\n",
    "\n",
    "    def is_clean_text(self, text):\n",
    "        not_nom_chars = r'\\sáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ'\n",
    "        pattern = re.compile(f'[{not_nom_chars}{re.escape(printable)}]')\n",
    "        return not bool(re.search(pattern, text.lower()))\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f'Samples count (not include Latin letters, numbers, punctuations):'\n",
    "            f'\\n- Number of images found: {len(self.img_paths)}'\n",
    "            f'\\n- Number of labels found: {len(self.labels)}'\n",
    "            f'\\n- Number of unique characters: {len(self.vocabs)}'\n",
    "            f'\\n- Characters present: {self.vocabs}'\n",
    "        )\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, dataset: DataImporter, img_size: tuple, padding_char, start_char='', end_char=''):\n",
    "        self.img_paths = dataset.img_paths\n",
    "        self.labels = dataset.labels\n",
    "        self.vocabs = dataset.vocabs\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.padding_char = padding_char\n",
    "        self.start_char = start_char\n",
    "        self.end_char = end_char\n",
    "\n",
    "        # Mapping characters to integers\n",
    "        vocabulary = list(self.vocabs)\n",
    "        if start_char != '' and end_char != '': vocabulary += [start_char, end_char]\n",
    "        self.char2num = tf.keras.layers.StringLookup(vocabulary=vocabulary, mask_token=padding_char)\n",
    "\n",
    "        # Mapping integers back to original characters\n",
    "        self.num2char = tf.keras.layers.StringLookup(\n",
    "            vocabulary = self.char2num.get_vocabulary(),\n",
    "            mask_token = padding_char,\n",
    "            invert = True,\n",
    "        )\n",
    "\n",
    "        self.max_length = max([len(label) for label in self.labels])\n",
    "        self.start_token, self.end_token = None, None\n",
    "        self.start_concat, self.end_concat = [], []\n",
    "        mask_idxs = [0, 1] # For [PAD] and [UNK] tokens\n",
    "\n",
    "        if self.start_char != '' and self.end_char != '':\n",
    "            self.start_token = self.char2num(start_char)\n",
    "            self.end_token = self.char2num(end_char)\n",
    "            self.start_concat = [self.start_token]\n",
    "            self.end_concat = [self.end_token]\n",
    "            self.max_length += 2 # For [START] and [END] tokens\n",
    "            mask_idxs.append(self.start_token)\n",
    "\n",
    "        # Prevent from generating padding, unknown, or start when using argmax in model.predict\n",
    "        token_mask = np.zeros([self.char2num.vocab_size()], dtype=bool)\n",
    "        token_mask[np.array(mask_idxs)] = True\n",
    "        self.token_mask = token_mask\n",
    "\n",
    "\n",
    "    def distortion_free_resize(self, image, align_top=True):\n",
    "        h, w = self.img_size\n",
    "        image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n",
    "\n",
    "        # Check the amount of padding needed to be done.\n",
    "        pad_height = h - tf.shape(image)[0]\n",
    "        pad_width = w - tf.shape(image)[1]\n",
    "        if pad_height == 0 and pad_width == 0: return image\n",
    "\n",
    "        # Only necessary if you want to do same amount of padding on both sides.\n",
    "        if pad_height % 2 != 0:\n",
    "            height = pad_height // 2\n",
    "            pad_height_top, pad_height_bottom = height + 1, height\n",
    "        else:\n",
    "            pad_height_top = pad_height_bottom = pad_height // 2\n",
    "\n",
    "        if pad_width % 2 != 0:\n",
    "            width = pad_width // 2\n",
    "            pad_width_left, pad_width_right = width + 1, width\n",
    "        else:\n",
    "            pad_width_left = pad_width_right = pad_width // 2\n",
    "\n",
    "        return tf.pad(image, paddings=[\n",
    "            [0, pad_height_top + pad_height_bottom] if align_top else [pad_height_top, pad_height_bottom],\n",
    "            [pad_width_left, pad_width_right],\n",
    "            [0, 0],\n",
    "        ], constant_values=255) # Pad with white color\n",
    "\n",
    "\n",
    "    def process_image(self, img_path, img_align_top=True):\n",
    "        image = tf.io.read_file(img_path)\n",
    "        image = tf.image.decode_jpeg(image, 3)\n",
    "        image = self.distortion_free_resize(image, img_align_top)\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        return image\n",
    "\n",
    "\n",
    "    def process_label(self, label):\n",
    "        label = self.char2num(tf.strings.unicode_split(label, input_encoding='UTF-8'))\n",
    "        label = tf.concat([self.start_concat, label, self.end_concat], 0)\n",
    "        label_length = tf.shape(label, tf.int64)[0]\n",
    "        label = tf.pad(\n",
    "            label,\n",
    "            paddings = [[0, self.max_length - label_length]],\n",
    "            constant_values = 0 # Pad with padding token\n",
    "        )\n",
    "        return label\n",
    "\n",
    "\n",
    "    def prepare_tf_dataset(self, idxs, batch_size, drop_remainder=False, img_align_top=True, use_cache=True):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.img_paths[idxs], self.labels[idxs])).map(\n",
    "            lambda img_path, label: (\n",
    "                self.process_image(img_path, img_align_top),\n",
    "                self.process_label(label)\n",
    "            ), num_parallel_calls = tf.data.AUTOTUNE\n",
    "        ).batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "        # When use .cache(), everything before is saved in the memory. It gives a\n",
    "        # significant boost in speed but only if you can get your hands on a larger RAM\n",
    "        if use_cache: dataset = dataset.cache()\n",
    "        return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "    def tokens2texts(self, batch_tokens, use_ctc_decode=False):\n",
    "        batch_texts = []\n",
    "        if use_ctc_decode:\n",
    "            batch_tokens = ctc_decode(batch_tokens, self.max_length)\n",
    "\n",
    "        # Iterate over the results and get back the text\n",
    "        for tokens in batch_tokens:\n",
    "            indices = tf.gather(tokens, tf.where(tf.logical_and(\n",
    "                tokens != 0, # For [PAD] token\n",
    "                tokens != -1 # For blank label if use_ctc_decode\n",
    "            )))\n",
    "\n",
    "            # Convert to string\n",
    "            text = tf.strings.reduce_join(self.num2char(indices))\n",
    "            text = text.numpy().decode('utf-8')\n",
    "            text = text.replace(self.start_char, '').replace(self.end_char, '')\n",
    "            batch_texts.append(text)\n",
    "        return batch_texts\n",
    "dataset = DataImporter(DATASET_DIR, ALL_TRANSCRIPTS_PATH, min_length=1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data constants and input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT, WIDTH = 432, 48\n",
    "PADDING_CHAR = '[PAD]'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = DataHandler(dataset, img_size=(HEIGHT, WIDTH), padding_char=PADDING_CHAR)\n",
    "NUM_VALIDATE = DataImporter(DATASET_DIR, VALID_TRANSCRIPTS_PATH, min_length=1).size\n",
    "VOCAB_SIZE = data_handler.char2num.vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "def draw_predicted_text(label, pred_label, fontdict, width, height):\n",
    "    label = label.replace('[UNK]', '?')\n",
    "    label_length, pred_length = len(label), len(pred_label)\n",
    "\n",
    "    if pred_label == label:\n",
    "        fontdict['color'] = 'green'\n",
    "        plt.text(width, 0, '\\n'.join(pred_label), fontdict=fontdict)\n",
    "        return\n",
    "\n",
    "    pred_start, start, end = 0, 0, 0\n",
    "    while start <= end < label_length:\n",
    "        text_y = end * height / label_length\n",
    "        actual_char = '[UNK]' if label[end] == '?' else label[end]\n",
    "\n",
    "        if label[start:end + 1] in pred_label[pred_start:pred_length]:\n",
    "            fontdict['color'] = 'dodgerblue'\n",
    "            plt.text(width, text_y, actual_char, fontdict=fontdict)\n",
    "        else:\n",
    "            if end < pred_length and end + 1 < label_length and pred_label[end] == label[end + 1]:\n",
    "                fontdict['color'] = 'gray'\n",
    "                plt.text(width, text_y, actual_char, fontdict=fontdict)\n",
    "            elif end < pred_length:\n",
    "                fontdict['color'] = 'red'\n",
    "                plt.text(width, text_y, pred_label[end], fontdict=fontdict)\n",
    "                fontdict['color'] = 'black'\n",
    "                plt.text(width * 2, text_y, actual_char, fontdict=fontdict)\n",
    "            else:\n",
    "                fontdict['color'] = 'gray'\n",
    "                plt.text(width, text_y, actual_char, fontdict=fontdict)\n",
    "\n",
    "            pred_start = end\n",
    "            start = end + 1\n",
    "        end += 1\n",
    "\n",
    "\n",
    "def visualize_images_labels(\n",
    "    img_paths,\n",
    "    labels, # shape == (batch_size, max_length)\n",
    "    pred_labels = None, # shape == (batch_size, max_length)\n",
    "    figsize = (15, 8),\n",
    "    subplot_size = (2, 8), # tuple: (rows, columns) to display\n",
    "    legend_loc = None, # Only for predictions,\n",
    "    annotate_loc = None, # Only for predictions\n",
    "    font_path = None\n",
    "):\n",
    "    nrows, ncols = subplot_size\n",
    "    num_of_labels = len(labels)\n",
    "    assert len(img_paths) == num_of_labels, 'img_paths and labels must have same number of items'\n",
    "    assert nrows * ncols <= num_of_labels, f'nrows * ncols must be <= {num_of_labels}'\n",
    "    fontdict = {\n",
    "        'fontproperties': FontProperties(fname=font_path),\n",
    "        'fontsize': 18,\n",
    "        'color': 'black',\n",
    "        'verticalalignment': 'top',\n",
    "        'horizontalalignment': 'left'\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(min(nrows * ncols, num_of_labels)):\n",
    "        plt.subplot(nrows, ncols, i + 1)\n",
    "        image, label = plt.imread(img_paths[i]), labels[i]\n",
    "        height, width, channel = image.shape\n",
    "        # print(height, width, channel)\n",
    "        plt.imshow(image)\n",
    "\n",
    "        fontdict['color'] = 'black'  # Reset the color\n",
    "        if pred_labels: draw_predicted_text(label, pred_labels[i], fontdict, width, height)\n",
    "        else: plt.text(width, 0, '\\n'.join(label), fontdict=fontdict)\n",
    "        plt.axis('off')\n",
    "\n",
    "    if legend_loc and annotate_loc and pred_labels:\n",
    "        plt.subplots_adjust(left=0, right=0.75)\n",
    "        plt.legend(handles=[\n",
    "            Patch(color='green', label='Full match'),\n",
    "            Patch(color='dodgerblue', label='Character match'),\n",
    "            Patch(color='red', label='Wrong prediction'),\n",
    "            Patch(color='black', label='Actual character'),\n",
    "            Patch(color='gray', label='Missing position'),\n",
    "        ], loc=legend_loc)\n",
    "\n",
    "        annotate_text = [f'{idx + 1:02d}. {text}' for idx, text in enumerate(pred_labels)]\n",
    "        plt.annotate(\n",
    "            f'Model predictions:\\n{chr(10).join(annotate_text)}',\n",
    "            fontproperties = FontProperties(fname=font_path),\n",
    "            xycoords = 'axes fraction',\n",
    "            fontsize = 14,\n",
    "            xy = annotate_loc,\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_training_results(history, save_name, figsize=(16, 14), subplot_size=(2, 2)):\n",
    "    nrows, ncols = subplot_size\n",
    "    if 'lr' in history.keys(): del history['lr']\n",
    "    assert nrows * ncols <= len(history), f'nrows * ncols must be <= {len(history)}'\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    for idx, name in enumerate(history):\n",
    "        if 'val' in name: continue\n",
    "        plt.subplot(nrows, ncols, idx + 1)\n",
    "        plt.plot(history[name], linestyle='solid', marker='o', color='crimson', label='Train')\n",
    "        plt.plot(history[f'val_{name}'], linestyle='solid', marker='o', color='dodgerblue', label='Validation')\n",
    "        plt.xlabel('Epochs', fontsize=14)\n",
    "        plt.ylabel(name, fontsize=14)\n",
    "\n",
    "        title = name.replace('acc', 'accuracy')\\\n",
    "                    .replace('seq_', 'sequence_')\\\n",
    "                    .replace('char_', 'character_')\\\n",
    "                    .replace('lev_', 'levenshtein_')\\\n",
    "                    .replace('edit_', 'levenshtein_')\\\n",
    "                    .replace('_', ' ').capitalize()\n",
    "        plt.title(title, fontsize=18)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    fig.savefig(save_name, bbox_inches='tight')\n",
    "    plt.show()\n",
    "visualize_images_labels(\n",
    "    dataset.img_paths,\n",
    "    dataset.labels,\n",
    "    figsize = (15, 15),\n",
    "    subplot_size = (2, 8),\n",
    "    font_path = FONT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Bidirectional, GRU\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Convolution2D, MaxPooling2D, BatchNormalization, Dense, Multiply,\n",
    "    Activation, LeakyReLU, Reshape, Permute, Lambda, RepeatVector\n",
    ")\n",
    "\n",
    "\n",
    "def custom_cnn(config, image_input, alpha=0):\n",
    "    # Generate Convolutional blocks by config\n",
    "    for idx, (block_name, block_config) in enumerate(config.items()):\n",
    "        num_conv, filters, pool_size = block_config.values()\n",
    "        for conv_idx in range(1, num_conv + 1):\n",
    "            x = Convolution2D(\n",
    "                filters = filters,\n",
    "                kernel_size = (3, 3) if pool_size else (2, 2),\n",
    "                padding = 'same' if pool_size else 'valid',\n",
    "                kernel_initializer = 'he_uniform',\n",
    "                name = f'{block_name}_conv{conv_idx}'\n",
    "            )(image_input if idx + conv_idx == 1 else x)\n",
    "\n",
    "            x = BatchNormalization(name=f'{block_name}_bn{conv_idx}')(x)\n",
    "            if alpha > 0: x = LeakyReLU(alpha, name=f'{block_name}_act{conv_idx}')(x)\n",
    "            else: x = Activation('relu', name=f'{block_name}_relu{conv_idx}')(x)\n",
    "\n",
    "        if pool_size is not None:\n",
    "            x = MaxPooling2D(pool_size, name=f'{block_name}_pool')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def reshape_features(last_cnn_layer, dim_to_keep=-1, name='cnn_features'):\n",
    "    # Reshape accordingly before passing the output to the RNN or Transformer\n",
    "    _, height, width, channel = last_cnn_layer.get_shape()\n",
    "    if dim_to_keep == 1:\n",
    "        target_shape = (height, width * channel)\n",
    "    elif dim_to_keep == 2:\n",
    "        target_shape = (width, height * channel)\n",
    "    elif dim_to_keep == 3 or dim_to_keep == -1:\n",
    "        target_shape = (height * width, channel)\n",
    "    else:\n",
    "        raise ValueError('Invalid dim_to_keep value')\n",
    "    return Reshape(target_shape=(target_shape), name=name)(last_cnn_layer)\n",
    "\n",
    "\n",
    "# https://pbcquoc.github.io/vietnamese-ocr (Vietnamese blog)\n",
    "def visual_attention(feature_maps):\n",
    "    _, timestep, input_dim = feature_maps.shape\n",
    "    a = Permute((2, 1), name='dim_switching1')(feature_maps)\n",
    "    a = Dense(timestep, activation='softmax', name='attention_scores')(a)\n",
    "    a = Lambda(lambda x: tf.reduce_sum(x, axis=1), name='dim_reduction')(a)\n",
    "    a = RepeatVector(input_dim, name='redistribute')(a)\n",
    "    a = Permute((2, 1), name='dim_switching2')(a)\n",
    "    return Multiply(name='context_vector')([feature_maps, a])\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, name='BahdanauAttention', **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(name=name, **kwargs)\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, enc_output, hidden):\n",
    "        # encoder output shape == (batch_size, receptive_size, channels)\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, receptive_size, units)\n",
    "        attention_hidden_layer = self.W1(enc_output) + self.W2(hidden_with_time_axis)\n",
    "        attention_hidden_layer = tf.nn.tanh(attention_hidden_layer)\n",
    "\n",
    "        # score shape == (batch_size, receptive_size, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, receptive_size, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, channels)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, name='AdditiveAttention', **kwargs):\n",
    "        super(AdditiveAttention, self).__init__(name=name, **kwargs)\n",
    "        self.W1 = Dense(units, use_bias=False)\n",
    "        self.W2 = Dense(units, use_bias=False)\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value):\n",
    "        w1_query, w2_key = self.W1(query), self.W2(value)\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crnn(imagenet_model=None, imagenet_output_layer=None, name='CRNN'):\n",
    "    # CNN layers\n",
    "    if imagenet_model: # Use Imagenet model as CNN layers\n",
    "        image_input = imagenet_model.input\n",
    "        imagenet_model.layers[0]._name = 'image'\n",
    "        x = imagenet_model.get_layer(imagenet_output_layer).output\n",
    "    else:\n",
    "        image_input = Input(shape=(HEIGHT, WIDTH, 3), dtype='float32', name='image')\n",
    "        conv_blocks_config = {\n",
    "            'block1': {'num_conv': 1, 'filters':  64, 'pool_size': (2, 2)},\n",
    "            'block2': {'num_conv': 1, 'filters': 128, 'pool_size': (2, 2)},\n",
    "            'block3': {'num_conv': 2, 'filters': 256, 'pool_size': (2, 2)},\n",
    "            'block4': {'num_conv': 2, 'filters': 512, 'pool_size': (2, 2)},\n",
    "\n",
    "            # Last Conv blocks with 2x2 kernel but without padding and pooling layer\n",
    "            'block5': {'num_conv': 2, 'filters': 512, 'pool_size': None},\n",
    "        }\n",
    "        x = custom_cnn(conv_blocks_config, image_input)\n",
    "\n",
    "    # Reshape accordingly before passing output to RNN\n",
    "    feature_maps = reshape_features(x, dim_to_keep=1, name='rnn_input')\n",
    "\n",
    "    # RNN layers\n",
    "    bigru1 = Bidirectional(GRU(256, return_sequences=True), name='bigru1')(feature_maps)\n",
    "    bigru2 = Bidirectional(GRU(256, return_sequences=True), name='bigru2')(bigru1)\n",
    "\n",
    "    # Output layer\n",
    "    y_pred = Dense(\n",
    "        units = VOCAB_SIZE + 1, # + 1 blank character for CTC loss\n",
    "        activation = 'softmax',\n",
    "        name = 'rnn_output'\n",
    "    )(bigru2)\n",
    "    return Model(inputs=image_input, outputs=y_pred, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "# https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model\n",
    "from abc import abstractmethod, ABCMeta # For define pure virtual functions\n",
    "\n",
    "\n",
    "def get_imagenet_model(model_name, input_shape):\n",
    "    # Pick a model from https://keras.io/api/applications\n",
    "    base_model = eval('tf.keras.applications.' + model_name)\n",
    "    return base_model(input_shape=input_shape, weights=None, include_top=False)\n",
    "\n",
    "\n",
    "class CustomTrainingModel(tf.keras.Model, metaclass=ABCMeta):\n",
    "    def __init__(self, data_handler=None, name='CustomTrainingModel', **kwargs):\n",
    "        super(CustomTrainingModel, self).__init__(name=name, **kwargs)\n",
    "        self.data_handler = data_handler\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        raise NotImplementedError() # To return model components when using clone_model\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config) # To clone model when using kfold training\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def _compute_loss_and_metrics(self, batch, is_training=False):\n",
    "        pass # Pure virtual functions => Must be overridden in the derived classes\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, batch_images):\n",
    "        pass # Pure virtual functions => Must be overridden in the derived classes\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, display_results = self._compute_loss_and_metrics(batch, is_training=True)\n",
    "\n",
    "        # Apply an optimization step\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return display_results\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, batch):\n",
    "        _, display_results = self._compute_loss_and_metrics(batch)\n",
    "        return display_results\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _update_metrics(self, batch):\n",
    "        batch_images, batch_tokens = batch\n",
    "        predictions = self.predict(batch_images)\n",
    "        self.compiled_metrics.update_state(batch_tokens, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _init_seq_tokens(self, batch_size, return_new_tokens=True):\n",
    "        seq_tokens = tf.fill([batch_size, self.data_handler.max_length], self.data_handler.start_token)\n",
    "        seq_tokens = tf.cast(seq_tokens, dtype=tf.int64)\n",
    "        new_tokens = tf.fill([batch_size, 1], self.data_handler.start_token)\n",
    "        new_tokens = tf.cast(new_tokens, dtype=tf.int64)\n",
    "\n",
    "        seq_tokens = update_tensor_column(seq_tokens, new_tokens, 0)\n",
    "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "        if not return_new_tokens: return seq_tokens, done\n",
    "        return seq_tokens, new_tokens, done\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _update_seq_tokens(self, y_pred, seq_tokens, done, pos_idx, return_new_tokens=True):\n",
    "        # Set the logits for all masked tokens to -inf, so they are never chosen\n",
    "        y_pred = tf.where(self.data_handler.token_mask, float('-inf'), y_pred)\n",
    "        new_tokens = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "        # Add batch dimension if it is not present after argmax\n",
    "        if tf.rank(new_tokens) == 1: new_tokens = tf.expand_dims(new_tokens, axis=1)\n",
    "\n",
    "        # Once a sequence is done it only produces padding token\n",
    "        new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "        seq_tokens = update_tensor_column(seq_tokens, new_tokens, pos_idx)\n",
    "\n",
    "        # If a sequence produces an `END_TOKEN`, set it `done` after that\n",
    "        done = done | (new_tokens == self.data_handler.end_token)\n",
    "        if not return_new_tokens: return seq_tokens, done\n",
    "        return seq_tokens, new_tokens, done\n",
    "\n",
    "\n",
    "class EncoderDecoderModel(CustomTrainingModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: tf.keras.Model,\n",
    "        decoder: tf.keras.Model,\n",
    "        data_handler = None, # DataHandler instance\n",
    "        dec_rnn_name = '', # Use if there is no rnn in the encoder\n",
    "        name = 'EncoderDecoderModel',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(EncoderDecoderModel, self).__init__(data_handler, name, **kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.dec_rnn_name = dec_rnn_name\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        return{\n",
    "            'encoder': clone_model(self.encoder),\n",
    "            'decoder': clone_model(self.decoder),\n",
    "            'dec_rnn_name': self.dec_rnn_name,\n",
    "            'data_handler': self.data_handler\n",
    "        }\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _compute_loss_and_metrics(self, batch, is_training=False):\n",
    "        batch_images, batch_tokens = batch\n",
    "        batch_size = batch_images.shape[0]\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        dec_input = tf.expand_dims([self.data_handler.start_token] * batch_size, 1)\n",
    "        if self.dec_rnn_name: # If there is no rnn in encoder, the hidden state will be initialized with 0\n",
    "            enc_output = self.encoder(batch_images, training=is_training)\n",
    "            dec_units = self.decoder.get_layer(self.dec_rnn_name).units\n",
    "            hidden = tf.zeros((batch_size, dec_units), dtype=tf.float32)\n",
    "        else: enc_output, hidden = self.encoder(batch_images, training=is_training)\n",
    "\n",
    "        for i in range(1, self.data_handler.max_length):\n",
    "            # Passing the features through the decoder\n",
    "            y_pred, hidden, _ = self.decoder([dec_input, enc_output, hidden], training=is_training)\n",
    "            loss += self.loss(batch_tokens[:, i], y_pred)\n",
    "            dec_input = tf.expand_dims(batch_tokens[:, i], 1) # Use teacher forcing\n",
    "\n",
    "        # Update training display result\n",
    "        metrics = self._update_metrics(batch)\n",
    "        return loss, {'loss': loss / self.data_handler.max_length, **metrics}\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, batch_images, return_attention=False):\n",
    "        batch_size = batch_images.shape[0]\n",
    "        seq_tokens, new_tokens, done = self._init_seq_tokens(batch_size)\n",
    "        attentions = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "        if self.dec_rnn_name:\n",
    "            enc_output = self.encoder(batch_images, training=False)\n",
    "            dec_units = self.decoder.get_layer(self.dec_rnn_name).units\n",
    "            hidden = tf.zeros((batch_size, dec_units), dtype=tf.float32)\n",
    "        else: enc_output, hidden = self.encoder(batch_images, training=False)\n",
    "\n",
    "        for i in range(1, self.data_handler.max_length):\n",
    "            y_pred, hidden, attention_weights = self.decoder([new_tokens, enc_output, hidden], training=False)\n",
    "            attentions = attentions.write(i - 1, attention_weights)\n",
    "            seq_tokens, new_tokens, done = self._update_seq_tokens(y_pred, seq_tokens, done, i)\n",
    "            if tf.executing_eagerly() and tf.reduce_all(done): break\n",
    "\n",
    "        if not return_attention: return seq_tokens\n",
    "        return seq_tokens, tf.transpose(tf.squeeze(attentions.stack()), [1, 0, 2])\n",
    "\n",
    "\n",
    "class EarlyBindingCaptioner(CustomTrainingModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnn_block: tf.keras.Model,\n",
    "        rnn_block: tf.keras.Model,\n",
    "        data_handler = None, # DataHandler instance\n",
    "        name = 'EarlyBindingCaptioner',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(EarlyBindingCaptioner, self).__init__(data_handler, name, **kwargs)\n",
    "        self.cnn_block = cnn_block\n",
    "        self.rnn_block = rnn_block\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'cnn_block': clone_model(self.cnn_block),\n",
    "            'rnn_block': clone_model(self.rnn_block),\n",
    "            'data_handler': self.data_handler,\n",
    "        }\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _loop(self, batch_images, batch_tokens=None, is_training=False):\n",
    "        batch_size = batch_images.shape[0]\n",
    "        seq_tokens, done = self._init_seq_tokens(batch_size, return_new_tokens=False)\n",
    "        features = self.cnn_block(batch_images, training=is_training)\n",
    "        loss = tf.constant(0.0)\n",
    "\n",
    "        for i in range(1, self.data_handler.max_length):\n",
    "            y_pred = self.rnn_block([seq_tokens[:, :-1], features], training=is_training)\n",
    "            seq_tokens, done = self._update_seq_tokens(y_pred, seq_tokens, done, i, return_new_tokens=False)\n",
    "            if batch_tokens is not None: loss += self.loss(batch_tokens[:, i], y_pred) # Is training\n",
    "            elif tf.executing_eagerly() and tf.reduce_all(done): break # Is predicting\n",
    "\n",
    "        if batch_tokens is None: return seq_tokens\n",
    "        return loss / self.data_handler.max_length\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def _compute_loss_and_metrics(self, batch, is_training=False):\n",
    "        batch_images, batch_tokens = batch\n",
    "        loss = self._loop(batch_images, batch_tokens, is_training)\n",
    "        metrics = self._update_metrics(batch) # Update training display result\n",
    "        return loss, {'loss': loss, **metrics}\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, batch_images):\n",
    "        return self._loop(batch_images)\n",
    "imagenet_model, imagenet_output_layer = None, None\n",
    "# # Pick a model from https://keras.io/api/applications\n",
    "# imagenet_model = get_imagenet_model('VGG16', (HEIGHT, WIDTH, 3))\n",
    "# imagenet_output_layer = 'block5_pool'\n",
    "# imagenet_model.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idxs = list(range(100))\n",
    "# valid_idxs = list(range(100, 150))\n",
    "# print('Number of training samples:', len(train_idxs))\n",
    "# print('Number of validate samples:', len(valid_idxs))\n",
    "\n",
    "train_idxs = list(range(dataset.size - NUM_VALIDATE))\n",
    "valid_idxs = list(range(train_idxs[-1] + 1, dataset.size))\n",
    "print('Number of training samples:', len(train_idxs))\n",
    "print('Number of validate samples:', len(valid_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2022)\n",
    "random.shuffle(train_idxs)\n",
    "\n",
    "# When run on a small RAM machine, you can set use_cache=False to\n",
    "# not run out of memory but it will slow down the training speed\n",
    "train_tf_dataset = data_handler.prepare_tf_dataset(train_idxs, BATCH_SIZE)\n",
    "valid_tf_dataset = data_handler.prepare_tf_dataset(valid_idxs, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    min_delta = 1e-3, # Change that less than 1e-3, will count as no improvement\n",
    "    patience = 5, # Stop if no improvement after 5 epochs\n",
    "    restore_best_weights = True, # Restore weights from the epoch with the best value\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, name='ctc_loss', **kwargs):\n",
    "        super(CTCLoss, self).__init__(name=name, **kwargs)\n",
    "        self.loss = tf.keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        label_length = tf.cast(y_true != 0, tf.int64)\n",
    "        label_length = tf.expand_dims(tf.reduce_sum(label_length, axis=-1), axis=1)\n",
    "\n",
    "        batch_length = tf.cast(tf.shape(y_true)[0], tf.int64)\n",
    "        pred_length = tf.cast(tf.shape(y_pred)[1], tf.int64)\n",
    "        pred_length *= tf.ones((batch_length, 1), tf.int64)\n",
    "        return self.loss(y_true, y_pred, pred_length, label_length)\n",
    "class SequenceAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_ctc_decode = False, # Need to decode predictions if CTC loss used,\n",
    "        name = 'seq_acc',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SequenceAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.use_ctc_decode = use_ctc_decode\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, **kwargs):\n",
    "        batch_size, max_length = tf.shape(y_true)[0], tf.shape(y_true)[1]\n",
    "        if self.use_ctc_decode: y_pred = ctc_decode(y_pred, max_length)\n",
    "\n",
    "        # Get a single batch and convert its labels to sparse tensors.\n",
    "        sparse_true = tokens2sparse(y_true)\n",
    "        sparse_pred = tokens2sparse(y_pred)\n",
    "\n",
    "        y_true = sparse2dense(sparse_true, [batch_size, max_length])\n",
    "        y_pred = sparse2dense(sparse_pred, [batch_size, max_length])\n",
    "\n",
    "        num_errors = tf.reduce_any(y_true != y_pred, axis=1)\n",
    "        num_errors = tf.reduce_sum(tf.cast(num_errors, tf.float32))\n",
    "        total = tf.cast(batch_size, tf.float32)\n",
    "\n",
    "        self.count.assign_add(total - num_errors)\n",
    "        self.total.assign_add(total)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.count, self.total)\n",
    "\n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.count.assign(0)\n",
    "        self.total.assign(0)\n",
    "class CharacterAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_ctc_decode = False, # Need to decode predictions if CTC loss used,\n",
    "        name = 'char_acc',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(CharacterAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.use_ctc_decode = use_ctc_decode\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, **kwargs):\n",
    "        batch_size, max_length = tf.shape(y_true)[0], tf.shape(y_true)[1]\n",
    "        if self.use_ctc_decode: y_pred = ctc_decode(y_pred, max_length)\n",
    "\n",
    "        num_errors = tf.logical_and(y_true != y_pred, y_true != 0)\n",
    "        num_errors = tf.reduce_sum(tf.cast(num_errors, tf.float32))\n",
    "        total = tf.reduce_sum(tf.cast(y_true != 0, tf.float32))\n",
    "\n",
    "        self.count.assign_add(total - num_errors)\n",
    "        self.total.assign_add(total)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.count, self.total)\n",
    "\n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.count.assign(0)\n",
    "        self.total.assign(0)\n",
    "class LevenshteinDistance(tf.keras.metrics.Metric):\n",
    "    def __init__(\n",
    "        self,\n",
    "        use_ctc_decode = False, # Need to decode predictions if CTC loss used,\n",
    "        normalize = False, # If True, this becomes Character Error Rate: CER = (S + D + I) / N\n",
    "        name = 'levenshtein_distance',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(LevenshteinDistance, self).__init__(name=name, **kwargs)\n",
    "        self.use_ctc_decode = use_ctc_decode\n",
    "        self.normalize = normalize\n",
    "        self.sum_distance = self.add_weight(name='sum_distance', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, **kwargs):\n",
    "        batch_size, max_length = tf.shape(y_true)[0], tf.shape(y_true)[1]\n",
    "        if self.use_ctc_decode: y_pred = ctc_decode(y_pred, max_length)\n",
    "\n",
    "        # Get a single batch and convert its labels to sparse tensors.\n",
    "        sparse_true = tokens2sparse(y_true)\n",
    "        sparse_pred = tokens2sparse(y_pred)\n",
    "\n",
    "        # Explain tf.edit_distance: https://stackoverflow.com/questions/51612489\n",
    "        edit_distances = tf.edit_distance(sparse_pred, sparse_true, normalize=self.normalize)\n",
    "        self.sum_distance.assign_add(tf.reduce_sum(edit_distances))\n",
    "        self.total.assign_add(tf.cast(batch_size, tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        # Computes and returns a scalar value for the metric\n",
    "        return tf.math.divide_no_nan(self.sum_distance, self.total)\n",
    "\n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sum_distance.assign(0)\n",
    "        self.total.assign(0)\n",
    "\n",
    "\n",
    "# https://github.com/solivr/tf-crnn/blob/master/tf_crnn/model.py#L157\n",
    "# The result of this function is the same as that of the\n",
    "# LevenshteinDistance metric above with normalize = True\n",
    "def warp_cer_metric(y_true, y_pred, use_ctc_decode=False):\n",
    "    ''' How to use:\n",
    "    from tensorflow.keras.metrics import MeanMetricWrapper\n",
    "    cer = MeanMetricWrapper(lambda y_true, y_pred: warp_cer_metric(\n",
    "        y_true, y_pred, use_ctc_decode=True\n",
    "    ), name='cer')\n",
    "    '''\n",
    "    if use_ctc_decode: y_pred = ctc_decode(y_pred, tf.shape(y_true)[1])\n",
    "    y_true = tf.cast(y_true, tf.int64)\n",
    "\n",
    "    # Get a single batch and convert its labels to sparse tensors.\n",
    "    sparse_true = tokens2sparse(y_true)\n",
    "    sparse_pred = tokens2sparse(y_pred)\n",
    "\n",
    "    # Explain tf.edit_distance: https://stackoverflow.com/questions/51612489\n",
    "    edit_distances = tf.edit_distance(sparse_pred, sparse_true, normalize=False)\n",
    "\n",
    "    # Compute edit distance and total chars count\n",
    "    sum_distance = tf.reduce_sum(edit_distances)\n",
    "    count_chars = tf.reduce_sum(tf.cast(y_true != 0, tf.float32))\n",
    "    return tf.math.divide_no_nan(sum_distance, count_chars, name='cer')\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "\n",
    "# Adadelta tends to benefit from higher initial learning rate values compared to\n",
    "# other optimizers. Here use 1.0 to match the exact form in the original paper\n",
    "LEARNING_RATE = 1.0\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q \"/content/drive/MyDrive/Thesis Resource/NomNaOCR/Data/weights.zip\" -d /content/Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_model = build_crnn(imagenet_model, imagenet_output_layer)\n",
    "reset_model.load_weights(f'/content/Weights/Text recognition/CRNNxCTC/Fine-tuning/finetune_CRNNxCTC.h5')\n",
    "reset_model.summary(line_length=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_model.compile(\n",
    "    optimizer = Adadelta(LEARNING_RATE),\n",
    "    loss = CTCLoss(),\n",
    "    metrics = [\n",
    "        SequenceAccuracy(use_ctc_decode=True),\n",
    "        CharacterAccuracy(use_ctc_decode=True),\n",
    "        LevenshteinDistance(use_ctc_decode=True, normalize=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (batch_images, batch_tokens) in enumerate(valid_tf_dataset.take(1)):\n",
    "    idxs_in_batch = valid_idxs[idx * BATCH_SIZE: (idx + 1) * BATCH_SIZE]\n",
    "    labels = data_handler.tokens2texts(batch_tokens)\n",
    "    pred_tokens = reset_model.predict(batch_images)\n",
    "    pred_labels = data_handler.tokens2texts(pred_tokens, use_ctc_decode=True)\n",
    "\n",
    "    visualize_images_labels(\n",
    "        img_paths = dataset.img_paths[idxs_in_batch],\n",
    "        labels = labels,\n",
    "        pred_labels = pred_labels,\n",
    "        figsize = (11.6, 30),\n",
    "        subplot_size = (4, 8),\n",
    "        legend_loc = (3.8, 4.38),\n",
    "        annotate_loc = (4, 2.75),\n",
    "        font_path = FONT_PATH,\n",
    "    )\n",
    "    print(\n",
    "        f'Batch {idx + 1:02d}:\\n'\n",
    "        f'- True: {dict(enumerate(labels, start=1))}\\n'\n",
    "        f'- Pred: {dict(enumerate(pred_labels, start=1))}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detail evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, dataset_dir, transcripts_path):\n",
    "        self.model = model\n",
    "        self.dataset = DataImporter(dataset_dir, transcripts_path, min_length=1)\n",
    "        self.tf_dataset = None\n",
    "\n",
    "\n",
    "    def evaluate(self, data_handler, batch_size, drop_remainder=False):\n",
    "        self.tf_dataset = tf.data.Dataset.from_tensor_slices((self.dataset.img_paths, self.dataset.labels))\n",
    "        self.tf_dataset = self.tf_dataset.map(\n",
    "            lambda img_path, label: (\n",
    "                data_handler.process_image(img_path),\n",
    "                data_handler.process_label(label)\n",
    "            ), num_parallel_calls = tf.data.AUTOTUNE\n",
    "        ).batch(batch_size, drop_remainder=drop_remainder)\n",
    "\n",
    "        self.data_handler = data_handler\n",
    "        self.tf_dataset = self.tf_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "        return self.model.evaluate(self.tf_dataset, return_dict=True)\n",
    "\n",
    "\n",
    "    def write_csv(self, file_name, use_ctc_decode=False):\n",
    "        assert self.tf_dataset, \"evaluate() method need to be run first\"\n",
    "        with open(file_name, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['img_paths', 'labels', 'pred_labels'])\n",
    "\n",
    "            for idx, (batch_images, batch_tokens) in tqdm(enumerate(self.tf_dataset)):\n",
    "                labels = self.data_handler.tokens2texts(batch_tokens)\n",
    "                pred_tokens = self.model.predict(batch_images)\n",
    "                pred_labels = self.data_handler.tokens2texts(pred_tokens, use_ctc_decode)\n",
    "\n",
    "                batch_size = len(batch_images)\n",
    "                paths = self.dataset.img_paths[idx * batch_size: (idx + 1) * batch_size]\n",
    "                paths = ['/'.join(os.path.abspath(path).split(os.path.sep)[-2:]) for path in paths]\n",
    "                writer.writerows(list(map(list, zip(*[paths, labels, pred_labels]))))\n",
    "GT10_TRANSCRIPTS_PATH = f'{DATASET_DIR}/Validate_gt10.txt'\n",
    "LTE10_TRANSCRIPTS_PATH = f'{DATASET_DIR}/Validate_lte10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt10_evaluator = Evaluator(reset_model, DATASET_DIR, GT10_TRANSCRIPTS_PATH)\n",
    "lte10_evaluator = Evaluator(reset_model, DATASET_DIR, LTE10_TRANSCRIPTS_PATH)\n",
    "df = pd.DataFrame([\n",
    "    reset_model.evaluate(valid_tf_dataset, return_dict=True),\n",
    "    gt10_evaluator.evaluate(data_handler, BATCH_SIZE),\n",
    "    lte10_evaluator.evaluate(data_handler, BATCH_SIZE),\n",
    "])\n",
    "df.index = ['Full', 'Length > 10', 'Length ≤ 10']\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
